{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project: Automatic Signal Detector_6-7_task.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "11guzedR5N3NP1sqS_ZEYsvylyHD2Qd2L",
      "authorship_tag": "ABX9TyPpCgK5Tf1nU1AAO0pV7FTg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lilly-yang/UCA--Machine_Learning_and_Computer_Vision/blob/main/Project_Automatic_Signal_Detector_6_7_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2iSp1qePAkj"
      },
      "source": [
        "# **Automatic Signal Detector**\n",
        "Task 6 - Task 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v38a8QCkPMJE"
      },
      "source": [
        "## README"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk7KE7c6PcRM"
      },
      "source": [
        "The goal of this progress is to make dataset and train MLP.\n",
        "- Save images with 2 kinds of size: 16x16, 224x224(have done in task3-task5)\n",
        "- Reshape 16x16 image to 1x256 array\n",
        "- Save each image as a line with Alphabet and 256 pixels in dataset.txt\n",
        "- Load and seperate dataset to train and validation data\n",
        "- Build, train and save model\n",
        "\n",
        "\n",
        "### Getting Started\n",
        "The dataset.txt file is in my Google driver will be used in MLP. Make sure you are mounted [my Google driver](https://drive.google.com/drive/folders/1SyAjYyn7sxlJULhwh1kkbA71xfAGkk2A?usp=sharing).\n",
        "\n",
        "_dataset.txt_\n",
        "\n",
        "\n",
        "### Running the progress\n",
        "Running cells in each part except these 2 cells which I marked as ***!!!don't need to run again***\n",
        "\n",
        "\n",
        "### Authors\n",
        "Li YANG   li.yang-li@etu.univ-cotedazur.fr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNmyPyaYvcXv"
      },
      "source": [
        "## Mounted Google Driver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfBLzs2PkdFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "805d2d1d-a108-4fb1-ab2e-8229a234263c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTxYuIpPYs4_"
      },
      "source": [
        "## Task6 - Create dataset\n",
        "\n",
        "I have 50-100 images for each **A, B, C, F, I, L, O, X, V, W and Y**\n",
        "  * for 'F, I, L, O, X, V, W', I made them with right hand by myself. And I expanded the border.\n",
        "  * for 'A, B, C, V, W', I got them from my mate and she made them by left hand.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh2ykMhlKiLT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "993e4361-47aa-4e45-fe64-70a9ec761105"
      },
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_A = cv.imread('/content/drive/My Drive/MLCV/train_val_16x16/A_0_16.jpg')\n",
        "img_B = cv.imread('/content/drive/My Drive/MLCV/train_val_16x16/B_30_16.jpg')\n",
        "img_C = cv.imread('/content/drive/My Drive/MLCV/train_val_16x16/C_31_16.jpg')\n",
        "img_F = cv.imread('/content/drive/My Drive/MLCV/train_val_16x16/F_38_16.jpg')\n",
        "img_I = cv.imread('/content/drive/My Drive/MLCV/train_val_16x16/I_76_16.jpg')\n",
        "img_L = cv.imread('/content/drive/My Drive/MLCV/train_val_16x16/L_28_16.jpg')\n",
        "img_O = cv.imread('/content/drive/My Drive/MLCV/train_val_16x16/O_29_16.jpg')\n",
        "img_V = cv.imread('/content/drive/My Drive/MLCV/train_val_16x16/V_2_16.jpg')\n",
        "img_W = cv.imread('/content/drive/My Drive/MLCV/train_val_16x16/W_45_16.jpg')\n",
        "img_X = cv.imread('/content/drive/My Drive/MLCV/train_val_16x16/X_63_16.jpg')\n",
        "img_Y = cv.imread('/content/drive/My Drive/MLCV/train_val_16x16/Y_16_16.jpg')\n",
        "\n",
        "img = [['F',img_F], ['I',img_I], ['L',img_L], ['O',img_O], ['X',img_X], ['Y',img_Y], ['A',img_A], ['B',img_B], ['C',img_C], ['V',img_V], ['W',img_W]]\n",
        "plt.figure()\n",
        "for i in range(11):\n",
        "  ax = plt.subplot(2,6,i+1)\n",
        "  ax.set_title(img[i][0])\n",
        "  plt.imshow(img[i][1])\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 11 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grPoJMj2Kg35"
      },
      "source": [
        "### Transform images to dataset.txt\n",
        "* Read these images  \n",
        "* Resize 16x16 image to an array of size (1,256) \n",
        "*   write in a text file, each line will have in the first position the uppercase letter and then the 256 values of the images\n",
        "\n",
        "    For example: `W,1,1,1,229,...,1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xGjgeL-lP3b"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "path = '/content/drive/My Drive/MLCV/train_val_16x16'\n",
        "dataset_file_path = '/content/drive/My Drive/MLCV/dataset.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogjcIKl2pUXp"
      },
      "source": [
        "\n",
        "\n",
        "*   ***!!!For codes below, don't need to run again***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5jMfJnJMmqZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "776e4de2-9047-44bb-c24a-9c4e8686d75d"
      },
      "source": [
        "f = open(dataset_file_path, \"a\")\n",
        "\n",
        "for inx, file_name in enumerate(os.listdir(path)):\n",
        "  ## Read image\n",
        "  I = Image.open(os.path.join(path, file_name))\n",
        "\n",
        "  ## Resize (16,16) image to (1,256)\n",
        "  I_arr = np.array(I)\n",
        "  I_arr = np.resize(I_arr, (1,256))\n",
        "\n",
        "  ## Reform image to str: W,1,1,226,...,1\n",
        "  I_str = [str(i) for i in list(I_arr[0])]\n",
        "  I_str = ','.join(I_str)  \n",
        "  data_temp = file_name[0]+','+I_str\n",
        "\n",
        "  ## Show progress\n",
        "  if not inx%50:\n",
        "    print('%d / %d \\n %s'%(inx, len(os.listdir(path)), data_temp))\n",
        "\n",
        "  ## Write alphabet and pixeles in to .txt file   \n",
        "  f.write(data_temp+'\\n')\n",
        "\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 / 996 \n",
            " X,0,0,0,1,0,0,0,1,0,0,1,0,1,0,0,0,0,2,0,0,2,0,3,0,0,0,0,0,0,1,0,0,1,0,2,0,2,2,0,2,0,0,1,1,0,0,1,2,1,1,0,4,0,0,0,1,1,0,0,0,3,0,0,0,0,0,0,0,2,2,1,0,0,0,1,0,0,1,3,0,0,4,0,0,0,0,255,255,0,3,1,0,6,0,0,3,0,0,1,0,3,4,0,23,255,0,0,0,0,0,1,3,1,0,0,3,0,255,255,237,255,255,2,0,0,3,0,0,2,0,0,0,14,209,254,255,255,255,0,0,0,1,0,1,0,0,0,2,1,255,254,255,40,34,2,3,0,0,5,0,2,0,0,0,2,0,38,255,255,253,29,3,1,0,0,2,0,0,3,0,1,4,0,113,255,255,37,0,1,0,4,0,1,1,0,5,0,0,2,0,22,255,36,1,0,3,0,2,0,0,0,2,3,0,3,0,0,43,17,0,0,0,0,0,2,0,2,0,0,2,0,1,1,0,3,0,0,2,2,0,1,0,0,2,0,0,2,0,1,0,0,1,0,0,0,0\n",
            "50 / 996 \n",
            " Y,2,0,0,0,1,0,0,0,0,4,0,0,0,0,1,0,0,2,2,1,0,0,1,0,0,0,1,2,0,0,0,0,1,0,0,0,4,2,0,0,1,1,3,0,0,1,2,0,1,0,0,0,0,0,0,1,0,0,0,0,2,1,0,2,0,0,0,0,1,0,3,0,0,1,3,2,0,0,2,0,2,0,1,0,255,0,0,0,0,1,0,0,7,2,0,1,0,0,3,0,37,2,0,4,3,0,4,0,0,2,0,1,1,0,0,4,183,188,255,253,34,124,0,1,255,0,3,0,0,1,0,0,0,255,255,211,35,34,0,76,0,1,1,0,0,0,0,1,0,255,254,255,30,36,253,253,4,0,0,0,0,0,0,2,0,27,255,255,38,31,35,71,0,0,0,1,0,2,0,0,2,2,250,255,33,32,34,1,0,2,0,0,0,1,0,3,1,0,2,255,32,168,36,0,0,3,0,3,2,0,0,0,1,3,0,0,35,34,20,0,3,0,1,0,0,3,0,2,0,0,0,1,0,3,0,4,0,0,0,3,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0\n",
            "100 / 996 \n",
            " F,0,3,0,1,2,0,0,0,0,0,1,0,0,1,0,0,0,1,1,0,0,0,1,0,0,2,0,2,0,0,5,0,1,0,0,0,1,0,0,2,1,0,0,0,2,1,0,1,0,0,0,5,0,2,3,0,24,1,0,0,1,1,0,3,1,2,0,0,0,0,0,255,37,0,5,0,2,0,0,0,0,0,7,0,0,3,203,234,101,1,0,5,0,1,4,255,2,0,0,2,0,1,195,253,254,79,1,0,0,0,1,254,0,0,4,0,2,117,255,255,254,255,255,0,1,0,2,0,1,2,0,0,0,255,255,254,255,38,139,2,0,0,0,0,0,0,3,2,12,255,253,255,255,253,255,255,1,0,0,0,2,0,0,0,19,254,253,255,255,254,255,0,2,0,1,0,0,2,0,1,15,255,253,15,253,255,255,1,0,0,2,1,0,2,1,0,2,0,255,255,255,253,0,0,2,0,0,0,5,0,0,2,0,0,0,0,12,4,0,0,1,3,0,3,0,0,124,0,0,2,97,18,1,1,0,4,0,0,0,0,0,13,0,1,3,14,10,0,0,0,0,0,0,1,0,0\n",
            "150 / 996 \n",
            " F,1,0,2,0,0,0,1,0,0,2,0,0,1,0,2,1,0,0,2,1,3,0,2,0,0,1,0,4,2,2,0,0,2,2,0,0,1,0,1,0,252,255,2,0,1,0,1,0,0,1,0,5,0,0,0,255,255,253,1,0,0,0,2,0,1,0,0,0,4,0,255,253,40,251,3,1,0,2,2,0,0,0,1,0,0,1,255,41,38,255,0,0,0,0,0,2,1,0,0,2,0,0,41,36,39,35,85,2,3,1,1,1,0,1,1,0,1,3,128,42,39,42,37,203,0,0,0,0,0,0,0,1,0,254,255,255,255,38,188,39,0,0,0,1,0,0,1,0,4,255,255,254,255,253,255,40,0,2,0,0,2,0,0,0,0,254,255,254,253,255,251,1,0,0,0,2,0,2,4,0,3,255,255,254,253,254,255,0,2,3,0,0,2,0,0,2,0,0,82,255,255,40,1,1,0,0,0,1,2,0,4,1,0,2,0,0,87,0,0,3,2,0,0,0,0,2,0,0,0,1,0,1,2,0,1,0,0,0,3,0,0,0,0,0,3,0,0,0,0,2,0,1,1,0,1,0\n",
            "200 / 996 \n",
            " I,1,0,0,2,0,0,4,0,2,0,1,0,0,0,2,0,0,1,0,2,0,0,0,4,0,0,1,0,2,0,2,0,0,1,2,1,1,255,0,0,0,0,0,4,0,0,1,0,1,0,0,0,0,255,0,3,2,1,1,0,1,1,0,2,0,2,0,0,0,255,0,0,0,0,2,2,3,3,0,0,0,0,0,2,0,253,0,0,0,0,2,0,0,0,5,0,0,2,0,0,4,255,0,218,3,0,253,2,4,0,0,0,2,0,1,2,7,255,255,254,253,255,255,0,0,1,1,0,1,1,0,1,158,255,17,255,254,255,19,4,0,0,2,0,0,2,3,0,255,253,254,255,255,255,255,102,2,0,0,1,59,9,17,0,253,255,255,253,255,253,255,114,0,2,0,0,0,2,0,0,20,255,255,109,255,35,255,254,1,0,2,4,2,0,2,0,22,253,253,255,253,255,214,44,0,0,0,0,0,0,0,0,22,254,255,255,255,253,253,0,4,0,2,1,0,5,1,0,23,253,255,171,253,255,255,1,0,1,0,0,2,0,2,69,18,255,255,255,255,255,0,1,2,0,1,0\n",
            "250 / 996 \n",
            " I,0,0,0,0,0,0,3,0,0,0,2,0,2,0,2,1,1,0,1,2,0,0,0,1,2,2,0,4,0,0,0,0,0,0,0,0,1,4,0,0,0,1,0,0,0,2,0,2,0,2,0,0,3,0,0,0,2,1,2,0,1,0,1,0,0,4,0,2,0,0,3,0,0,0,0,0,0,2,0,0,1,0,0,0,2,3,102,0,0,1,2,0,0,0,0,2,1,1,0,2,0,0,254,2,156,255,0,8,3,2,0,0,0,1,3,0,1,0,255,254,255,255,182,255,2,0,1,1,0,1,0,0,1,1,254,255,35,36,1,36,255,0,1,0,1,1,0,5,0,253,255,255,253,35,253,34,51,0,0,1,0,0,0,0,1,0,0,1,0,38,2,0,0,4,2,1,1,0,2,0,1,160,94,11,37,31,0,0,1,0,2,0,0,2,0,0,0,4,255,255,253,255,255,0,1,2,0,3,0,0,1,0,1,0,0,0,3,253,0,2,0,1,0,0,2,1,0,3,0,0,4,0,0,1,0,0,0,0,1,2,0,0,0,0,1,2,0,0,0,1,0,2,0,0,1,0\n",
            "300 / 996 \n",
            " O,0,3,0,0,1,2,0,0,0,0,0,1,0,0,1,1,1,0,5,1,0,0,1,0,2,0,0,0,0,1,1,0,1,0,0,2,1,0,4,0,0,0,3,0,0,1,0,6,0,0,3,0,2,0,1,0,0,5,0,0,0,0,1,0,0,4,0,0,1,2,1,2,2,0,0,5,1,0,1,0,0,0,0,3,0,0,75,255,43,17,1,0,3,0,0,2,0,1,2,0,3,136,255,253,40,175,255,255,1,1,3,0,1,0,0,1,0,18,255,43,221,0,1,66,1,1,0,0,0,3,0,2,0,255,253,42,254,0,53,55,122,2,0,1,1,1,1,0,0,176,143,41,255,177,254,254,3,0,2,0,0,2,0,5,0,252,255,255,255,191,255,4,0,0,1,1,0,0,0,0,0,255,255,255,255,0,255,0,5,0,1,0,0,0,5,0,3,0,0,19,255,255,0,1,0,1,0,3,4,0,0,0,0,1,4,1,0,0,1,0,0,4,0,1,0,3,0,0,3,1,0,0,0,2,0,0,3,0,0,0,0,1,0,0,0,0,0,2,0,0,2,0,2,0,0,1\n",
            "350 / 996 \n",
            " O,1,0,2,0,1,2,0,0,0,1,0,1,0,2,2,0,0,4,0,0,1,0,0,2,0,2,0,0,3,0,0,4,0,3,3,0,0,3,0,0,0,3,0,5,0,0,4,0,4,0,0,0,0,2,0,0,3,0,0,0,0,2,0,0,0,1,1,1,2,0,1,2,255,255,3,0,0,3,0,0,3,0,0,0,0,1,0,251,255,254,253,11,1,0,0,2,0,3,0,1,0,0,255,24,255,1,41,45,0,0,0,0,2,0,0,0,66,255,15,253,0,0,0,40,0,0,2,0,0,0,0,0,20,255,254,254,0,0,135,255,0,1,0,0,0,1,2,0,19,15,255,255,255,255,135,0,0,1,0,0,1,0,0,0,19,25,0,255,251,254,3,0,4,0,4,0,0,2,2,0,23,255,2,255,255,255,0,2,0,0,0,1,1,0,0,0,1,252,40,255,253,0,1,0,1,0,1,1,0,0,1,0,1,0,253,255,255,0,1,0,0,1,4,0,1,0,3,1,0,1,2,0,0,0,0,2,0,0,0,1,0,2,0,0,1,1,0,1,1,0,1,0,0,4,0,1\n",
            "400 / 996 \n",
            " L,2,0,0,4,0,0,2,0,0,0,1,0,0,2,0,1,0,3,1,0,2,0,0,0,0,0,0,0,0,0,0,0,0,3,0,1,1,0,1,0,1,1,2,0,2,0,0,5,0,0,2,0,0,2,0,5,181,0,0,0,0,5,0,0,0,0,1,0,0,2,0,0,255,0,0,2,0,0,0,1,0,2,0,2,0,0,0,5,255,213,0,1,0,0,5,0,0,0,1,0,0,0,0,0,253,255,5,0,0,0,0,0,2,0,2,0,1,59,255,255,255,255,0,0,0,0,3,0,2,0,3,3,0,255,255,254,255,254,1,0,255,22,5,0,0,3,2,0,1,255,255,255,252,255,255,255,0,3,0,3,1,1,0,2,2,255,253,254,255,255,253,0,1,0,1,0,0,0,0,0,1,0,254,255,253,254,0,3,1,0,2,1,0,0,2,0,0,1,2,254,100,0,0,0,0,0,0,0,0,3,0,1,0,0,0,1,196,1,0,1,2,0,0,1,0,1,0,0,3,0,0,0,2,0,2,0,0,1,0,0,1,0,3,0,1,0,0,1,0,0,0,0,3,0,0,1\n",
            "450 / 996 \n",
            " L,0,0,3,0,0,0,0,1,0,3,0,0,3,0,1,1,1,2,0,3,0,0,4,0,2,0,1,1,0,1,0,2,0,0,0,0,3,0,1,0,0,0,3,0,0,0,2,0,0,3,0,4,0,0,0,0,255,141,0,2,0,3,0,2,3,0,0,1,0,0,0,0,4,255,0,0,0,0,0,0,0,2,1,0,0,2,0,0,0,255,2,0,2,0,0,2,1,0,1,0,3,0,1,0,0,255,0,0,0,0,3,0,0,2,0,0,0,247,252,255,255,255,0,2,1,1,0,2,1,0,1,0,255,255,255,255,255,255,0,0,175,255,0,0,0,0,1,0,0,255,225,255,255,254,253,255,205,0,3,2,1,1,0,1,0,255,100,255,255,255,255,253,2,0,0,0,0,0,1,1,1,255,254,252,251,146,212,0,0,2,4,0,3,0,0,0,1,0,1,255,255,186,0,2,0,1,0,4,0,0,3,1,0,2,0,0,229,1,0,0,0,4,0,0,2,0,0,0,1,0,4,0,1,0,2,0,1,0,1,1,0,0,1,0,2,0,0,1,1,0,0,1,0,4,0,0\n",
            "500 / 996 \n",
            " V,223,255,130,15,1,0,0,0,255,255,253,2,0,1,1,1,0,255,204,130,0,1,0,1,125,191,255,0,0,3,0,0,0,255,147,190,128,3,0,255,255,129,252,1,0,0,1,2,0,1,123,240,255,0,0,131,255,189,33,3,0,0,1,0,1,0,49,255,255,127,3,125,126,130,253,0,2,1,0,0,0,0,0,255,126,127,125,130,128,128,129,255,2,0,2,1,0,0,1,0,128,192,133,156,254,255,252,128,127,0,0,0,0,2,0,3,222,253,158,255,255,91,255,128,254,192,2,0,0,0,1,142,255,254,255,255,253,255,252,160,255,255,253,2,1,1,0,191,160,254,253,255,255,255,255,255,255,255,0,0,0,0,2,129,253,254,255,252,253,255,255,251,254,255,0,0,0,2,1,0,255,255,255,255,255,254,253,255,255,30,1,0,0,1,0,0,255,253,255,255,252,254,255,255,0,2,0,2,3,1,5,0,32,255,0,33,191,255,255,0,5,0,0,0,0,0,0,3,1,0,0,0,142,157,2,2,0,5,0,0,0,4,0,0,1,0,1,1,0,0,1,0,0,0,0,2\n",
            "550 / 996 \n",
            " V,0,0,0,1,0,1,0,0,0,2,0,21,0,1,2,0,2,2,0,255,128,0,3,1,0,0,255,255,1,0,0,1,0,0,0,251,255,114,0,0,0,255,253,255,0,3,0,0,0,0,2,164,131,129,0,2,0,128,125,96,3,0,0,1,2,0,0,255,123,126,3,0,209,126,255,3,0,3,2,0,0,0,2,77,255,146,254,0,127,131,159,0,0,3,0,2,0,1,3,0,127,127,125,130,128,127,130,251,2,3,0,0,0,1,0,1,128,131,126,130,255,129,127,255,151,0,0,0,0,0,0,0,128,127,255,126,255,252,254,255,255,252,2,0,1,0,3,1,130,126,255,255,0,1,232,0,1,255,0,0,0,0,0,125,133,236,252,255,0,255,255,255,252,0,3,1,2,0,0,128,251,255,255,1,255,253,255,255,255,0,0,0,1,0,0,255,255,160,0,0,255,255,250,255,0,2,4,0,0,0,2,0,128,3,0,0,174,255,0,1,2,0,0,0,0,2,0,3,0,0,6,0,1,3,0,0,0,2,0,3,1,1,0,0,1,2,0,0,0,0,0,3,0,0,2,0\n",
            "600 / 996 \n",
            " B,0,1,0,3,1,0,1,0,0,1,0,0,0,0,0,1,2,0,0,252,254,206,252,2,2,0,1,0,3,1,0,3,0,207,255,254,255,199,255,193,255,255,1,0,0,0,0,0,0,254,207,254,202,211,204,211,254,255,244,0,0,0,0,2,0,0,208,255,207,242,0,208,254,235,252,1,0,2,0,0,0,4,252,251,255,217,255,255,254,206,255,255,1,0,1,0,0,0,255,255,204,208,205,208,250,206,205,209,253,1,0,0,0,0,0,208,255,206,209,254,253,255,255,255,255,255,0,0,1,0,0,202,255,254,254,255,206,209,253,254,255,255,0,1,0,0,255,255,253,255,253,207,206,208,208,208,226,223,250,1,2,0,209,207,205,217,211,204,209,206,206,207,220,253,255,0,0,0,1,160,255,255,206,208,205,207,205,217,209,251,254,0,1,3,0,1,0,0,206,255,255,208,219,254,255,254,209,252,0,0,0,0,1,0,80,167,254,252,209,224,254,231,206,89,1,0,0,0,1,2,0,0,1,2,1,228,202,205,209,0,1,0,0,1,0,0,1,0,0,0,0,0,2,0,0,1\n",
            "650 / 996 \n",
            " B,0,0,0,0,0,2,0,1,0,0,2,0,0,0,0,1,4,0,1,1,0,255,255,231,222,1,0,0,0,1,2,0,0,1,1,206,252,255,255,255,254,229,2,0,0,2,0,3,2,0,0,209,255,205,207,255,207,255,207,253,94,0,1,0,0,0,0,206,209,209,206,252,207,207,254,222,255,1,0,4,1,0,1,241,254,206,210,255,253,236,208,210,250,1,3,0,2,0,0,210,208,207,206,219,209,208,218,255,255,0,1,0,0,0,3,253,206,255,254,255,255,252,255,254,255,4,0,1,0,2,0,207,255,255,253,255,254,210,205,254,208,255,0,3,1,0,255,253,255,254,255,254,254,208,207,210,255,255,1,0,0,149,253,253,255,255,206,245,255,252,210,253,218,254,5,0,0,0,1,255,223,255,253,216,208,208,252,254,255,1,0,1,3,0,0,3,252,255,252,255,254,255,255,255,252,0,0,1,0,2,0,1,0,206,255,255,224,255,255,255,209,2,3,0,0,2,0,2,0,3,0,0,206,35,0,1,1,0,0,2,1,1,0,0,1,0,0,0,0,0,0,2,1,0,1,0\n",
            "700 / 996 \n",
            " C,0,0,0,1,1,0,0,0,0,0,4,0,0,0,0,1,2,0,0,0,0,0,3,0,3,0,0,1,0,0,0,0,1,0,251,232,232,247,231,230,227,5,0,0,4,0,0,0,0,5,0,255,255,252,255,254,252,255,228,2,1,0,2,0,0,1,2,0,0,3,0,93,255,251,255,234,0,0,2,0,3,0,1,0,0,2,222,254,71,0,0,231,146,3,0,0,0,0,3,0,3,0,1,0,0,3,0,232,254,0,1,0,1,0,0,0,0,1,88,255,1,0,0,255,254,0,0,1,230,255,0,0,0,229,229,246,0,1,2,221,255,0,0,2,0,255,252,255,232,230,235,226,148,0,251,255,254,0,3,0,0,25,255,254,227,232,225,233,228,230,233,252,255,223,0,2,0,0,0,255,255,231,230,230,231,228,245,251,253,0,2,0,2,0,0,4,0,0,255,254,229,255,252,3,0,0,1,0,0,0,0,0,0,4,0,0,4,0,0,0,0,0,0,1,2,0,0,2,0,1,0,2,0,1,2,0,1,0,0,0,0,0,0,0,0,0,2,0,0,1,0,2,0,0,1,0\n",
            "750 / 996 \n",
            " C,0,0,0,1,0,1,0,0,0,2,0,0,1,0,1,0,0,1,3,2,10,0,0,0,0,0,1,0,2,0,1,0,0,1,226,0,226,232,230,230,232,0,1,2,0,0,0,1,0,2,255,33,234,253,255,252,248,228,226,0,0,0,6,0,1,0,1,0,0,1,0,255,3,255,252,228,2,1,0,0,0,4,0,0,2,0,0,59,0,0,101,230,1,0,0,3,1,0,0,2,0,2,3,0,1,2,0,242,232,1,0,0,2,0,3,0,0,0,0,1,0,0,0,255,235,1,0,1,0,0,2,1,0,0,0,0,1,0,0,1,254,1,0,1,240,242,229,91,94,4,229,242,0,4,0,0,255,229,0,0,0,254,255,255,228,228,254,254,255,0,254,4,252,248,0,0,0,2,0,254,255,255,233,229,230,255,255,160,252,229,3,0,0,5,0,3,1,0,0,3,235,231,0,1,39,0,0,0,2,0,3,0,0,0,0,2,0,0,0,0,0,0,0,3,0,2,0,1,1,1,3,0,2,1,1,0,0,3,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,3,0\n",
            "800 / 996 \n",
            " W,0,0,0,1,1,1,235,235,235,0,0,0,2,2,2,0,0,0,0,0,0,1,1,1,231,231,231,229,229,229,0,0,0,4,4,4,1,1,1,227,227,227,230,230,230,43,43,43,234,234,234,2,2,2,0,0,0,1,1,1,0,0,0,2,2,2,5,5,5,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,5,5,5,0,0,0,0,0,0,201,201,201,1,1,1,1,1,1,231,231,231,228,228,228,254,254,254,255,255,255,230,230,230,1,1,1,0,0,0,0,0,0,3,3,3,215,215,215,0,0,0,0,0,0,4,4,4,0,0,0,238,238,238,255,255,255,0,0,0,10,10,10,255,255,255,242,242,242,228,228,228,4,4,4,1,1,1,2,2,2,245,245,245,253,253,253,0,0,0,0,0,0,0,0,0,2,2,2,255,255,255,225,225,225,232,232,232,2,2,2,241,241,241,231,231,231,230,230,230,0,0,0,0,0,0,255,255,255,253,253,253,238,238,238,2,2,2,1,1,1,1,1,1,1,1,1,0,0,0,248,248,248,255,255,255,238\n",
            "850 / 996 \n",
            " W,0,0,0,3,3,3,1,1,1,0,0,0,0,0,0,3,3,3,0,0,0,1,1,1,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,2,2,2,0,0,0,0,0,0,255,255,255,224,224,224,233,233,233,0,0,0,2,2,2,0,0,0,0,0,0,232,232,232,3,3,3,1,1,1,0,0,0,3,3,3,0,0,0,254,254,254,1,1,1,0,0,0,207,207,207,241,241,241,251,251,251,3,3,3,0,0,0,1,1,1,231,231,231,255,255,255,242,242,242,0,0,0,0,0,0,2,2,2,0,0,0,231,231,231,249,249,249,0,0,0,0,0,0,230,230,230,231,231,231,235,235,235,1,1,1,0,0,0,233,233,233,231,231,231,159,159,159,3,3,3,0,0,0,0,0,0,3,3,3,4,4,4,232,232,232,231,231,231,231,231,231,0,0,0,229,229,229,233,233,233,246,246,246,0,0,0,255,255,255,233,233,233,0,0,0,2,2,2,2,2,2,5,5,5,0,0,0,0,0,0,150,150,150,255,255,255,254,254,254,255,255,255,254\n",
            "900 / 996 \n",
            " X,0,0,0,0,0,1,0,1,0,2,0,0,4,0,2,1,1,0,1,1,1,0,1,0,1,0,2,0,0,4,1,0,0,1,0,0,1,2,1,0,0,0,0,1,1,0,0,0,0,2,1,3,0,0,0,1,0,4,0,1,2,0,2,2,1,0,0,3,0,0,4,0,1,0,1,1,0,2,0,0,1,5,0,2,2,0,0,0,0,23,24,0,0,0,4,1,0,0,0,1,0,4,12,255,0,0,80,1,2,4,0,0,0,1,0,0,1,0,255,255,255,255,255,1,0,0,0,1,1,0,0,2,0,253,242,255,255,255,255,0,0,0,2,1,0,0,0,0,2,1,254,253,254,255,3,0,0,2,0,0,0,0,4,0,1,0,253,255,254,197,0,1,2,255,247,1,2,0,0,1,0,4,255,254,255,1,0,0,1,0,255,0,0,1,0,1,0,0,255,254,255,0,2,2,0,5,0,0,0,1,1,0,0,3,194,0,14,2,0,1,1,0,2,2,0,0,0,4,0,0,0,0,0,0,0,0,0,5,2,0,0,2,1,0,1,0,0,1,1,0,1,0,1,1,0,3\n",
            "950 / 996 \n",
            " A,2,0,0,0,1,0,2,0,2,0,0,0,4,0,3,0,0,1,179,1,0,0,0,0,0,2,2,2,0,1,0,0,0,0,175,177,206,135,255,255,51,132,255,0,1,0,2,2,0,1,176,178,178,251,255,227,255,253,254,176,0,2,0,0,0,0,189,175,253,255,196,255,252,255,254,178,255,178,0,0,0,0,254,177,181,177,177,175,180,254,177,254,253,177,0,0,0,0,255,212,177,175,178,177,176,179,177,255,179,177,0,1,0,0,189,255,254,224,114,177,177,176,176,179,176,177,178,0,0,2,254,252,255,254,255,179,175,181,176,176,180,241,178,0,0,0,27,176,249,255,252,251,255,252,254,255,252,208,176,0,0,0,3,177,175,177,179,181,177,179,255,255,255,175,174,1,1,0,0,0,255,253,175,176,255,174,255,254,255,188,0,2,0,2,0,0,134,179,176,179,177,255,175,176,176,178,0,0,0,0,2,1,0,176,181,174,177,181,176,179,37,0,0,1,1,0,0,2,0,3,253,178,178,176,115,0,0,2,0,0,1,0,5,0,2,0,0,231,0,2,1,0,1,0,1,0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4I8dzoKaJNH"
      },
      "source": [
        "### Shuffle the lines of dataset file\n",
        "\n",
        "\n",
        "\n",
        "* Coyied from CV course home page\n",
        "*   ***!!!For codes below, don't need to run again***\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dInRhmOcMl-F"
      },
      "source": [
        "import random\n",
        "lines = open(dataset_file_path).readlines()\n",
        "random.shuffle(lines)\n",
        "open(dataset_file_path, 'w').writelines(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K600iH3haWD3"
      },
      "source": [
        "## Task7 - Create MLP\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL0WTelQa_cz"
      },
      "source": [
        "### Load dataset for trainnig \n",
        "*   read the dataset file\n",
        "*   seperate it to two arrays: samples and letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHafZ8PCWq9r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b09a1f9b-3a45-4681-cf4c-44ff8ff1e32e"
      },
      "source": [
        "def load_dataset(dataset_file_path):\n",
        "    a = np.loadtxt(dataset_file_path, delimiter=',', converters={ 0 : lambda ch : ord(ch)-ord('A') })\n",
        "    samples, letters = a[:,1:], a[:,0]\n",
        "    return samples, letters\n",
        "\n",
        "samples, letters = load_dataset(dataset_file_path)\n",
        "print(samples[:3])\n",
        "print(letters[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  2.   0.   0.   2.   0.   1.   0.   0.   0.   2.   0.   0.   0.   4.\n",
            "    0.   1.   0.   1.   1.   0.   0. 244. 253. 250. 208.   1.   2.   0.\n",
            "    0.   0.   0.   0.   0.   1.   1. 207. 255. 253. 211. 255. 255. 254.\n",
            "    0.   3.   0.   0.   4.   2.   0.   0.   1. 208. 207. 229. 207. 243.\n",
            "  217. 255. 235. 254. 251.   2.   0.   0.   0.   2.   1. 205. 209. 255.\n",
            "  204. 255. 207. 207. 255. 207. 253.   0.   4.   0.   0.   0.   0. 255.\n",
            "  253. 255. 255. 253. 209. 212. 253. 255. 253.   0.   0.   0.   0.   5.\n",
            "    2. 205. 207. 204. 209. 208. 206. 205. 211. 255. 255.   0.   1.   0.\n",
            "    0.   0.   0. 244. 254. 255. 253. 255. 255. 229. 251. 254. 255.   0.\n",
            "    0.   1.   1.   1.   0. 208. 253. 255. 235. 254. 255. 255. 253. 208.\n",
            "  254. 210.   0.   1.   0.   2. 229. 255. 255. 255. 253. 255. 206. 203.\n",
            "  209. 208. 210. 253.   0.   0.   2.   0. 255. 251. 254. 252. 207. 255.\n",
            "  255. 255. 252. 253. 226. 175.   3.   0.   0.   0.   0. 244. 255. 255.\n",
            "  255. 254. 203. 255. 208. 255. 255.   0.   0.   3.   1.   0.   4.   0.\n",
            "  252. 255. 254. 254. 255. 253. 255. 252. 255.   0.   0.   0.   0.   3.\n",
            "    0.   0.   4. 228. 252. 255. 255. 253. 255. 255. 254.   2.   1.   0.\n",
            "    0.   1.   0.   1.   0.   3.   1.   0. 255. 208. 231.   0.   0.   0.\n",
            "    0.   1.   1.   0.   2.   1.   0.   1.   0.   1.   0.   0.   0.   2.\n",
            "    0.   1.   1.   0.]\n",
            " [  1.   0.   0.   2.   0.   0.   4.   0.   2.   0.   1.   0.   0.   0.\n",
            "    2.   0.   0.   1.   0.   2.   0.   0.   0.   4.   0.   0.   1.   0.\n",
            "    2.   0.   2.   0.   0.   1.   2.   1.   1. 255.   0.   0.   0.   0.\n",
            "    0.   4.   0.   0.   1.   0.   1.   0.   0.   0.   0. 255.   0.   3.\n",
            "    2.   1.   1.   0.   1.   1.   0.   2.   0.   2.   0.   0.   0. 255.\n",
            "    0.   0.   0.   0.   2.   2.   3.   3.   0.   0.   0.   0.   0.   2.\n",
            "    0. 253.   0.   0.   0.   0.   2.   0.   0.   0.   5.   0.   0.   2.\n",
            "    0.   0.   4. 255.   0. 218.   3.   0. 253.   2.   4.   0.   0.   0.\n",
            "    2.   0.   1.   2.   7. 255. 255. 254. 253. 255. 255.   0.   0.   1.\n",
            "    1.   0.   1.   1.   0.   1. 158. 255.  17. 255. 254. 255.  19.   4.\n",
            "    0.   0.   2.   0.   0.   2.   3.   0. 255. 253. 254. 255. 255. 255.\n",
            "  255. 102.   2.   0.   0.   1.  59.   9.  17.   0. 253. 255. 255. 253.\n",
            "  255. 253. 255. 114.   0.   2.   0.   0.   0.   2.   0.   0.  20. 255.\n",
            "  255. 109. 255.  35. 255. 254.   1.   0.   2.   4.   2.   0.   2.   0.\n",
            "   22. 253. 253. 255. 253. 255. 214.  44.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.  22. 254. 255. 255. 255. 253. 253.   0.   4.   0.   2.   1.\n",
            "    0.   5.   1.   0.  23. 253. 255. 171. 253. 255. 255.   1.   0.   1.\n",
            "    0.   0.   2.   0.   2.  69.  18. 255. 255. 255. 255. 255.   0.   1.\n",
            "    2.   0.   1.   0.]\n",
            " [  0.   3.   0.   0.   3.   0.   0.   1.   0.   0.   0.   1.   0.   0.\n",
            "    2.   0.   2.   0.   0.   2.   0.   4.   0.   0.   0.   0.   3.   0.\n",
            "    1.   0.   0.   1.   0.   0.   0.   2.   0.   0.   4.   0.   0.   1.\n",
            "    0.   0.   0.   2.   0.   0.   2.   1.   0.   1.   0.   2.   2.   0.\n",
            "    1.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   1.\n",
            "    0.   2.   0.   2.   2.   0.   3.   0.   0.   0.   0.   3.   1.   1.\n",
            "  186. 255.   3.   0.   0.   0.   0.   0.   0.   0.   5.   0.   1.   1.\n",
            "    0.   0.   2. 255.   0. 136.   0. 203. 255.   0.   2.   0.   0.   0.\n",
            "    0.   1.   1.   2.   0. 255. 252. 255. 255. 255.   0. 255.   0.   1.\n",
            "    0.   1.   1.   2.   0.   1. 184. 255. 255. 255. 255. 208.  73. 255.\n",
            "   99.   2.   0.   0.   1.   0.   7.   0.   0. 255. 252. 255.  35.  37.\n",
            "  254.  35.  35.   0.   0.   0.   0.   1.   0.   0.   1. 255. 255. 255.\n",
            "  252. 255.  24.  31.   3.   0.   2.   3.   0.   1.   2.   0.   0.  17.\n",
            "  255. 253. 255.  37.   0.   1.   0.   2.   0.   0.   0.   0.   0.   2.\n",
            "  186.  21. 255. 138. 254.  35.   0.   4.   2.   0.   3.   0.   2.   0.\n",
            "    0.   1.   0.   0.   0. 195. 255.  31.   1.   0.   0.   2.   0.   0.\n",
            "    0.   0.   4.   1.   0.   2.   2.   0.   0.   2.   1.   2.   2.   1.\n",
            "    0.   1.   0.   2.   0.   0.   0.   1.   0.   3.   1.   0.   1.   0.\n",
            "    1.   0.   2.   0.]]\n",
            "[ 1.  8.  8. 24.  1.  8.  2.  8. 21. 24.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65X6eiJCbuTz"
      },
      "source": [
        "### set partecepante for dataset that will use for training and validation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SnHnqLFWrjT"
      },
      "source": [
        "train_ratio = 0.8\n",
        "n_train_samples = int(len(samples) * train_ratio)\n",
        "x_train, y_train = samples[:n_train_samples], letters[:n_train_samples]\n",
        "x_val, y_val = samples[n_train_samples:], letters[n_train_samples:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR3YHg5lcBAj"
      },
      "source": [
        "### Use Keras library to build and train the MLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9n8rh3ecEw-"
      },
      "source": [
        "\n",
        "\n",
        "####   Build MLP\n",
        "\n",
        "    Setting parameters and build ML model framwork\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRWLiLw-WunO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "10b9ea98-4bae-40ba-b319-d75d2ed8aac7"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "\n",
        "## the biggest value of label is 'Y' = 24, so the num_classes should be 25(include 0)\n",
        "num_classes = 25\n",
        "epochs = 500\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_val = x_val.astype('float32')\n",
        "\n",
        "## 0-1 normolazition\n",
        "x_train /= 255\n",
        "x_val /= 255\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_val.shape[0], 'test samples')\n",
        "\n",
        "## convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "\n",
        "## model-sequential\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(256,)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "# model.add(Dense(32, activation='relu'))\n",
        "# model.add(Dropout(0.3))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "## print model formate\n",
        "model.summary()\n",
        "\n",
        "## set optimizer, especially the learn rate\n",
        "sgd = keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "## compile model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "796 train samples\n",
            "200 test samples\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_38 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 25)                1625      \n",
            "=================================================================\n",
            "Total params: 42,777\n",
            "Trainable params: 42,777\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOQXEm_rcldM"
      },
      "source": [
        "#### train model\n",
        "\n",
        "    In the traning process, i have changed the epoches, number of dense blockes and it's layers, the dropout ratio, and the learn rate a lot of times. \n",
        "    PS: Because the results of each tuning are not fully recorded during the tuning process, the results given below (except the final result) are given an impression.\n",
        "\n",
        "1.   for the original version got from CV course home page, Sorry I dont remember the details, but I think val_loss> 1 is a calculation error\n",
        "```\n",
        "Validation loss: 0.0*\n",
        "Validation accuracy: 1.*\n",
        "```\n",
        "2.   then I added one more dense block and changed the dropout ration for each dense bolok, which I don't get better validation loss and accuracy.\n",
        "\n",
        "3. then i increased epoches from 10-100, i noticed there are some improve but not that much.\n",
        "\n",
        "4. then i incrased epoches from 100 to 1000, the loss are decreasing but val-loss decreased firstly then increasing when trainning. In other words, the model is over fitted\n",
        "```\n",
        "Validation loss: 0.7*\n",
        "Validation accuracy: 0.9*\n",
        "```\n",
        "\n",
        "5. Then i uncommented the last Dense block and reset the optimizer which learn rate is 0.001. I got a satisfactory result\n",
        "```\n",
        "Validation loss: 0.13428444683551788\n",
        "Validation accuracy: 0.9599999785423279\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-XhJHCick2g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b54fb1ec-8557-415c-af4a-92496ee1c135"
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val))\n",
        "score = model.evaluate(x_val, y_val, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 796 samples, validate on 200 samples\n",
            "Epoch 1/500\n",
            "796/796 [==============================] - 0s 187us/step - loss: 3.2419 - accuracy: 0.0528 - val_loss: 3.0522 - val_accuracy: 0.1500\n",
            "Epoch 2/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 3.0452 - accuracy: 0.0917 - val_loss: 2.8495 - val_accuracy: 0.2450\n",
            "Epoch 3/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 2.8336 - accuracy: 0.1495 - val_loss: 2.6528 - val_accuracy: 0.2500\n",
            "Epoch 4/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 2.6643 - accuracy: 0.1935 - val_loss: 2.4598 - val_accuracy: 0.3050\n",
            "Epoch 5/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 2.5204 - accuracy: 0.2085 - val_loss: 2.2788 - val_accuracy: 0.3800\n",
            "Epoch 6/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 2.4306 - accuracy: 0.2261 - val_loss: 2.1254 - val_accuracy: 0.4850\n",
            "Epoch 7/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 2.2870 - accuracy: 0.2651 - val_loss: 1.9884 - val_accuracy: 0.5800\n",
            "Epoch 8/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 2.2067 - accuracy: 0.2902 - val_loss: 1.8641 - val_accuracy: 0.6200\n",
            "Epoch 9/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 2.0810 - accuracy: 0.3153 - val_loss: 1.7377 - val_accuracy: 0.6600\n",
            "Epoch 10/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 1.9362 - accuracy: 0.3920 - val_loss: 1.6240 - val_accuracy: 0.6650\n",
            "Epoch 11/500\n",
            "796/796 [==============================] - 0s 64us/step - loss: 1.9222 - accuracy: 0.3568 - val_loss: 1.5360 - val_accuracy: 0.6950\n",
            "Epoch 12/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 1.7897 - accuracy: 0.3995 - val_loss: 1.4498 - val_accuracy: 0.7200\n",
            "Epoch 13/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 1.7791 - accuracy: 0.4058 - val_loss: 1.3729 - val_accuracy: 0.7100\n",
            "Epoch 14/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 1.6790 - accuracy: 0.4447 - val_loss: 1.3046 - val_accuracy: 0.7200\n",
            "Epoch 15/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 1.6043 - accuracy: 0.4749 - val_loss: 1.2406 - val_accuracy: 0.7250\n",
            "Epoch 16/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 1.5586 - accuracy: 0.4585 - val_loss: 1.1906 - val_accuracy: 0.7500\n",
            "Epoch 17/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 1.5199 - accuracy: 0.4749 - val_loss: 1.1491 - val_accuracy: 0.7500\n",
            "Epoch 18/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 1.4746 - accuracy: 0.4987 - val_loss: 1.1076 - val_accuracy: 0.7800\n",
            "Epoch 19/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 1.4480 - accuracy: 0.4975 - val_loss: 1.0755 - val_accuracy: 0.7800\n",
            "Epoch 20/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 1.4005 - accuracy: 0.5075 - val_loss: 1.0438 - val_accuracy: 0.7950\n",
            "Epoch 21/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 1.3697 - accuracy: 0.5427 - val_loss: 1.0072 - val_accuracy: 0.8150\n",
            "Epoch 22/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 1.3374 - accuracy: 0.5201 - val_loss: 0.9840 - val_accuracy: 0.8150\n",
            "Epoch 23/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 1.2412 - accuracy: 0.5817 - val_loss: 0.9528 - val_accuracy: 0.8300\n",
            "Epoch 24/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 1.2547 - accuracy: 0.5528 - val_loss: 0.9306 - val_accuracy: 0.8300\n",
            "Epoch 25/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 1.2034 - accuracy: 0.5829 - val_loss: 0.9076 - val_accuracy: 0.8400\n",
            "Epoch 26/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 1.1852 - accuracy: 0.5829 - val_loss: 0.8846 - val_accuracy: 0.8500\n",
            "Epoch 27/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 1.1681 - accuracy: 0.5867 - val_loss: 0.8673 - val_accuracy: 0.8300\n",
            "Epoch 28/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 1.1604 - accuracy: 0.6055 - val_loss: 0.8482 - val_accuracy: 0.8450\n",
            "Epoch 29/500\n",
            "796/796 [==============================] - 0s 63us/step - loss: 1.1602 - accuracy: 0.5967 - val_loss: 0.8317 - val_accuracy: 0.8500\n",
            "Epoch 30/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 1.1148 - accuracy: 0.6193 - val_loss: 0.8168 - val_accuracy: 0.8500\n",
            "Epoch 31/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 1.0631 - accuracy: 0.6394 - val_loss: 0.7960 - val_accuracy: 0.8650\n",
            "Epoch 32/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 1.1006 - accuracy: 0.6156 - val_loss: 0.7826 - val_accuracy: 0.8600\n",
            "Epoch 33/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 1.0672 - accuracy: 0.6156 - val_loss: 0.7714 - val_accuracy: 0.8700\n",
            "Epoch 34/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 1.0092 - accuracy: 0.6558 - val_loss: 0.7548 - val_accuracy: 0.8650\n",
            "Epoch 35/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 1.0102 - accuracy: 0.6445 - val_loss: 0.7388 - val_accuracy: 0.8800\n",
            "Epoch 36/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.9864 - accuracy: 0.6420 - val_loss: 0.7252 - val_accuracy: 0.8750\n",
            "Epoch 37/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.9900 - accuracy: 0.6633 - val_loss: 0.7065 - val_accuracy: 0.8750\n",
            "Epoch 38/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.9701 - accuracy: 0.6595 - val_loss: 0.6935 - val_accuracy: 0.8700\n",
            "Epoch 39/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.9201 - accuracy: 0.7035 - val_loss: 0.6862 - val_accuracy: 0.8850\n",
            "Epoch 40/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.9382 - accuracy: 0.6910 - val_loss: 0.6736 - val_accuracy: 0.8900\n",
            "Epoch 41/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.9152 - accuracy: 0.6960 - val_loss: 0.6579 - val_accuracy: 0.8900\n",
            "Epoch 42/500\n",
            "796/796 [==============================] - 0s 63us/step - loss: 0.8648 - accuracy: 0.7098 - val_loss: 0.6434 - val_accuracy: 0.8900\n",
            "Epoch 43/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.8628 - accuracy: 0.7073 - val_loss: 0.6293 - val_accuracy: 0.8900\n",
            "Epoch 44/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.8655 - accuracy: 0.7173 - val_loss: 0.6209 - val_accuracy: 0.8950\n",
            "Epoch 45/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.8454 - accuracy: 0.7173 - val_loss: 0.6109 - val_accuracy: 0.8900\n",
            "Epoch 46/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.8520 - accuracy: 0.7010 - val_loss: 0.6034 - val_accuracy: 0.9000\n",
            "Epoch 47/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.7917 - accuracy: 0.7324 - val_loss: 0.5911 - val_accuracy: 0.9050\n",
            "Epoch 48/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.8219 - accuracy: 0.7261 - val_loss: 0.5827 - val_accuracy: 0.8950\n",
            "Epoch 49/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.8019 - accuracy: 0.7437 - val_loss: 0.5704 - val_accuracy: 0.8900\n",
            "Epoch 50/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.8007 - accuracy: 0.7249 - val_loss: 0.5581 - val_accuracy: 0.8950\n",
            "Epoch 51/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.7604 - accuracy: 0.7387 - val_loss: 0.5496 - val_accuracy: 0.9050\n",
            "Epoch 52/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.7765 - accuracy: 0.7487 - val_loss: 0.5433 - val_accuracy: 0.8950\n",
            "Epoch 53/500\n",
            "796/796 [==============================] - 0s 68us/step - loss: 0.7625 - accuracy: 0.7588 - val_loss: 0.5351 - val_accuracy: 0.9050\n",
            "Epoch 54/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.7690 - accuracy: 0.7374 - val_loss: 0.5238 - val_accuracy: 0.9000\n",
            "Epoch 55/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.7034 - accuracy: 0.7676 - val_loss: 0.5151 - val_accuracy: 0.9050\n",
            "Epoch 56/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.7177 - accuracy: 0.7601 - val_loss: 0.5075 - val_accuracy: 0.9050\n",
            "Epoch 57/500\n",
            "796/796 [==============================] - 0s 64us/step - loss: 0.7032 - accuracy: 0.7739 - val_loss: 0.5002 - val_accuracy: 0.9050\n",
            "Epoch 58/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.7267 - accuracy: 0.7450 - val_loss: 0.4935 - val_accuracy: 0.9100\n",
            "Epoch 59/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.6694 - accuracy: 0.7827 - val_loss: 0.4876 - val_accuracy: 0.9100\n",
            "Epoch 60/500\n",
            "796/796 [==============================] - 0s 64us/step - loss: 0.6559 - accuracy: 0.8003 - val_loss: 0.4761 - val_accuracy: 0.9050\n",
            "Epoch 61/500\n",
            "796/796 [==============================] - 0s 63us/step - loss: 0.6450 - accuracy: 0.8028 - val_loss: 0.4669 - val_accuracy: 0.9050\n",
            "Epoch 62/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.6506 - accuracy: 0.7990 - val_loss: 0.4583 - val_accuracy: 0.9050\n",
            "Epoch 63/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.6608 - accuracy: 0.7726 - val_loss: 0.4541 - val_accuracy: 0.9100\n",
            "Epoch 64/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.6180 - accuracy: 0.8103 - val_loss: 0.4503 - val_accuracy: 0.9100\n",
            "Epoch 65/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.6181 - accuracy: 0.8028 - val_loss: 0.4411 - val_accuracy: 0.9050\n",
            "Epoch 66/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.6033 - accuracy: 0.8191 - val_loss: 0.4355 - val_accuracy: 0.9150\n",
            "Epoch 67/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.6210 - accuracy: 0.8028 - val_loss: 0.4301 - val_accuracy: 0.9100\n",
            "Epoch 68/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.6230 - accuracy: 0.8078 - val_loss: 0.4206 - val_accuracy: 0.9100\n",
            "Epoch 69/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.5851 - accuracy: 0.8141 - val_loss: 0.4130 - val_accuracy: 0.9150\n",
            "Epoch 70/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.6344 - accuracy: 0.7965 - val_loss: 0.4110 - val_accuracy: 0.9150\n",
            "Epoch 71/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.6006 - accuracy: 0.8028 - val_loss: 0.4096 - val_accuracy: 0.9200\n",
            "Epoch 72/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.5594 - accuracy: 0.8216 - val_loss: 0.4032 - val_accuracy: 0.9150\n",
            "Epoch 73/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.5790 - accuracy: 0.8153 - val_loss: 0.4003 - val_accuracy: 0.9150\n",
            "Epoch 74/500\n",
            "796/796 [==============================] - 0s 63us/step - loss: 0.6020 - accuracy: 0.7990 - val_loss: 0.3972 - val_accuracy: 0.9200\n",
            "Epoch 75/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.5739 - accuracy: 0.8241 - val_loss: 0.3891 - val_accuracy: 0.9200\n",
            "Epoch 76/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.5284 - accuracy: 0.8505 - val_loss: 0.3851 - val_accuracy: 0.9150\n",
            "Epoch 77/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.5215 - accuracy: 0.8329 - val_loss: 0.3736 - val_accuracy: 0.9150\n",
            "Epoch 78/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.5177 - accuracy: 0.8530 - val_loss: 0.3700 - val_accuracy: 0.9200\n",
            "Epoch 79/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.5385 - accuracy: 0.8229 - val_loss: 0.3689 - val_accuracy: 0.9200\n",
            "Epoch 80/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.5242 - accuracy: 0.8191 - val_loss: 0.3655 - val_accuracy: 0.9200\n",
            "Epoch 81/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.4986 - accuracy: 0.8392 - val_loss: 0.3607 - val_accuracy: 0.9200\n",
            "Epoch 82/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.5099 - accuracy: 0.8279 - val_loss: 0.3567 - val_accuracy: 0.9200\n",
            "Epoch 83/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.4817 - accuracy: 0.8543 - val_loss: 0.3463 - val_accuracy: 0.9250\n",
            "Epoch 84/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.4516 - accuracy: 0.8693 - val_loss: 0.3426 - val_accuracy: 0.9250\n",
            "Epoch 85/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.5472 - accuracy: 0.8241 - val_loss: 0.3394 - val_accuracy: 0.9200\n",
            "Epoch 86/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.5119 - accuracy: 0.8279 - val_loss: 0.3351 - val_accuracy: 0.9200\n",
            "Epoch 87/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.4759 - accuracy: 0.8606 - val_loss: 0.3331 - val_accuracy: 0.9250\n",
            "Epoch 88/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.4677 - accuracy: 0.8530 - val_loss: 0.3288 - val_accuracy: 0.9250\n",
            "Epoch 89/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.4879 - accuracy: 0.8379 - val_loss: 0.3290 - val_accuracy: 0.9300\n",
            "Epoch 90/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.4573 - accuracy: 0.8580 - val_loss: 0.3241 - val_accuracy: 0.9300\n",
            "Epoch 91/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.4289 - accuracy: 0.8656 - val_loss: 0.3195 - val_accuracy: 0.9200\n",
            "Epoch 92/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.4488 - accuracy: 0.8706 - val_loss: 0.3149 - val_accuracy: 0.9300\n",
            "Epoch 93/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.4546 - accuracy: 0.8518 - val_loss: 0.3102 - val_accuracy: 0.9300\n",
            "Epoch 94/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.4397 - accuracy: 0.8505 - val_loss: 0.3111 - val_accuracy: 0.9300\n",
            "Epoch 95/500\n",
            "796/796 [==============================] - 0s 63us/step - loss: 0.4272 - accuracy: 0.8668 - val_loss: 0.3066 - val_accuracy: 0.9250\n",
            "Epoch 96/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.4538 - accuracy: 0.8467 - val_loss: 0.3018 - val_accuracy: 0.9300\n",
            "Epoch 97/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.4441 - accuracy: 0.8693 - val_loss: 0.3027 - val_accuracy: 0.9300\n",
            "Epoch 98/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.4166 - accuracy: 0.8593 - val_loss: 0.2992 - val_accuracy: 0.9300\n",
            "Epoch 99/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.4078 - accuracy: 0.8706 - val_loss: 0.2960 - val_accuracy: 0.9300\n",
            "Epoch 100/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.4042 - accuracy: 0.8832 - val_loss: 0.2949 - val_accuracy: 0.9300\n",
            "Epoch 101/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.4422 - accuracy: 0.8631 - val_loss: 0.2891 - val_accuracy: 0.9300\n",
            "Epoch 102/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.4210 - accuracy: 0.8756 - val_loss: 0.2863 - val_accuracy: 0.9300\n",
            "Epoch 103/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.4019 - accuracy: 0.8719 - val_loss: 0.2830 - val_accuracy: 0.9300\n",
            "Epoch 104/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.4178 - accuracy: 0.8668 - val_loss: 0.2815 - val_accuracy: 0.9300\n",
            "Epoch 105/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.4080 - accuracy: 0.8844 - val_loss: 0.2772 - val_accuracy: 0.9300\n",
            "Epoch 106/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.3780 - accuracy: 0.8794 - val_loss: 0.2759 - val_accuracy: 0.9300\n",
            "Epoch 107/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.3900 - accuracy: 0.8719 - val_loss: 0.2742 - val_accuracy: 0.9300\n",
            "Epoch 108/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.3803 - accuracy: 0.8932 - val_loss: 0.2731 - val_accuracy: 0.9300\n",
            "Epoch 109/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.3736 - accuracy: 0.8995 - val_loss: 0.2724 - val_accuracy: 0.9250\n",
            "Epoch 110/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.3911 - accuracy: 0.8693 - val_loss: 0.2676 - val_accuracy: 0.9300\n",
            "Epoch 111/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.3650 - accuracy: 0.8932 - val_loss: 0.2638 - val_accuracy: 0.9300\n",
            "Epoch 112/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.3569 - accuracy: 0.8970 - val_loss: 0.2619 - val_accuracy: 0.9300\n",
            "Epoch 113/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.3762 - accuracy: 0.8807 - val_loss: 0.2578 - val_accuracy: 0.9300\n",
            "Epoch 114/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.3425 - accuracy: 0.9070 - val_loss: 0.2530 - val_accuracy: 0.9300\n",
            "Epoch 115/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.3639 - accuracy: 0.8882 - val_loss: 0.2536 - val_accuracy: 0.9300\n",
            "Epoch 116/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.3523 - accuracy: 0.8982 - val_loss: 0.2524 - val_accuracy: 0.9300\n",
            "Epoch 117/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.3637 - accuracy: 0.8869 - val_loss: 0.2531 - val_accuracy: 0.9300\n",
            "Epoch 118/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.3571 - accuracy: 0.8920 - val_loss: 0.2514 - val_accuracy: 0.9300\n",
            "Epoch 119/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.3431 - accuracy: 0.9008 - val_loss: 0.2477 - val_accuracy: 0.9350\n",
            "Epoch 120/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.3715 - accuracy: 0.8844 - val_loss: 0.2466 - val_accuracy: 0.9300\n",
            "Epoch 121/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.3242 - accuracy: 0.9108 - val_loss: 0.2419 - val_accuracy: 0.9300\n",
            "Epoch 122/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.3526 - accuracy: 0.8920 - val_loss: 0.2414 - val_accuracy: 0.9300\n",
            "Epoch 123/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.3339 - accuracy: 0.8932 - val_loss: 0.2423 - val_accuracy: 0.9300\n",
            "Epoch 124/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.3448 - accuracy: 0.8894 - val_loss: 0.2368 - val_accuracy: 0.9300\n",
            "Epoch 125/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.3398 - accuracy: 0.9020 - val_loss: 0.2352 - val_accuracy: 0.9250\n",
            "Epoch 126/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.3192 - accuracy: 0.8945 - val_loss: 0.2359 - val_accuracy: 0.9300\n",
            "Epoch 127/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.3365 - accuracy: 0.8945 - val_loss: 0.2341 - val_accuracy: 0.9300\n",
            "Epoch 128/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.3368 - accuracy: 0.8982 - val_loss: 0.2337 - val_accuracy: 0.9300\n",
            "Epoch 129/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.3213 - accuracy: 0.9033 - val_loss: 0.2296 - val_accuracy: 0.9300\n",
            "Epoch 130/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.3188 - accuracy: 0.8957 - val_loss: 0.2313 - val_accuracy: 0.9300\n",
            "Epoch 131/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.3143 - accuracy: 0.9008 - val_loss: 0.2291 - val_accuracy: 0.9300\n",
            "Epoch 132/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.3164 - accuracy: 0.9008 - val_loss: 0.2286 - val_accuracy: 0.9300\n",
            "Epoch 133/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2995 - accuracy: 0.9259 - val_loss: 0.2251 - val_accuracy: 0.9300\n",
            "Epoch 134/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.3348 - accuracy: 0.9108 - val_loss: 0.2245 - val_accuracy: 0.9300\n",
            "Epoch 135/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.3062 - accuracy: 0.9070 - val_loss: 0.2205 - val_accuracy: 0.9350\n",
            "Epoch 136/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.2681 - accuracy: 0.9234 - val_loss: 0.2190 - val_accuracy: 0.9350\n",
            "Epoch 137/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.3149 - accuracy: 0.9008 - val_loss: 0.2187 - val_accuracy: 0.9300\n",
            "Epoch 138/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2706 - accuracy: 0.9108 - val_loss: 0.2194 - val_accuracy: 0.9400\n",
            "Epoch 139/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.2812 - accuracy: 0.9121 - val_loss: 0.2152 - val_accuracy: 0.9300\n",
            "Epoch 140/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2842 - accuracy: 0.9171 - val_loss: 0.2154 - val_accuracy: 0.9300\n",
            "Epoch 141/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.2799 - accuracy: 0.9209 - val_loss: 0.2139 - val_accuracy: 0.9350\n",
            "Epoch 142/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.3047 - accuracy: 0.9020 - val_loss: 0.2179 - val_accuracy: 0.9300\n",
            "Epoch 143/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.3228 - accuracy: 0.9058 - val_loss: 0.2164 - val_accuracy: 0.9350\n",
            "Epoch 144/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.2624 - accuracy: 0.9259 - val_loss: 0.2110 - val_accuracy: 0.9450\n",
            "Epoch 145/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.2587 - accuracy: 0.9284 - val_loss: 0.2109 - val_accuracy: 0.9350\n",
            "Epoch 146/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.2665 - accuracy: 0.9171 - val_loss: 0.2117 - val_accuracy: 0.9400\n",
            "Epoch 147/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.2948 - accuracy: 0.9070 - val_loss: 0.2106 - val_accuracy: 0.9400\n",
            "Epoch 148/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.2688 - accuracy: 0.9221 - val_loss: 0.2112 - val_accuracy: 0.9350\n",
            "Epoch 149/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2702 - accuracy: 0.9171 - val_loss: 0.2070 - val_accuracy: 0.9350\n",
            "Epoch 150/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2658 - accuracy: 0.9171 - val_loss: 0.2038 - val_accuracy: 0.9350\n",
            "Epoch 151/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.2825 - accuracy: 0.9221 - val_loss: 0.2031 - val_accuracy: 0.9400\n",
            "Epoch 152/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.2669 - accuracy: 0.9221 - val_loss: 0.2044 - val_accuracy: 0.9400\n",
            "Epoch 153/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.2689 - accuracy: 0.9296 - val_loss: 0.2015 - val_accuracy: 0.9400\n",
            "Epoch 154/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.2761 - accuracy: 0.9322 - val_loss: 0.1988 - val_accuracy: 0.9400\n",
            "Epoch 155/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.2807 - accuracy: 0.9083 - val_loss: 0.1995 - val_accuracy: 0.9400\n",
            "Epoch 156/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.3012 - accuracy: 0.9108 - val_loss: 0.1980 - val_accuracy: 0.9400\n",
            "Epoch 157/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2443 - accuracy: 0.9410 - val_loss: 0.1973 - val_accuracy: 0.9450\n",
            "Epoch 158/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.2743 - accuracy: 0.9133 - val_loss: 0.1943 - val_accuracy: 0.9450\n",
            "Epoch 159/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2546 - accuracy: 0.9359 - val_loss: 0.1960 - val_accuracy: 0.9450\n",
            "Epoch 160/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.2351 - accuracy: 0.9347 - val_loss: 0.1917 - val_accuracy: 0.9450\n",
            "Epoch 161/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2462 - accuracy: 0.9322 - val_loss: 0.1929 - val_accuracy: 0.9450\n",
            "Epoch 162/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.2623 - accuracy: 0.9108 - val_loss: 0.1952 - val_accuracy: 0.9400\n",
            "Epoch 163/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.2600 - accuracy: 0.9146 - val_loss: 0.1942 - val_accuracy: 0.9450\n",
            "Epoch 164/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2136 - accuracy: 0.9347 - val_loss: 0.1883 - val_accuracy: 0.9450\n",
            "Epoch 165/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.2501 - accuracy: 0.9271 - val_loss: 0.1879 - val_accuracy: 0.9450\n",
            "Epoch 166/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.2483 - accuracy: 0.9284 - val_loss: 0.1887 - val_accuracy: 0.9450\n",
            "Epoch 167/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2515 - accuracy: 0.9221 - val_loss: 0.1912 - val_accuracy: 0.9400\n",
            "Epoch 168/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.2446 - accuracy: 0.9271 - val_loss: 0.1925 - val_accuracy: 0.9400\n",
            "Epoch 169/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.2304 - accuracy: 0.9271 - val_loss: 0.1871 - val_accuracy: 0.9450\n",
            "Epoch 170/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2467 - accuracy: 0.9309 - val_loss: 0.1845 - val_accuracy: 0.9400\n",
            "Epoch 171/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.2472 - accuracy: 0.9296 - val_loss: 0.1853 - val_accuracy: 0.9400\n",
            "Epoch 172/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.2614 - accuracy: 0.9171 - val_loss: 0.1849 - val_accuracy: 0.9450\n",
            "Epoch 173/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.2257 - accuracy: 0.9347 - val_loss: 0.1817 - val_accuracy: 0.9400\n",
            "Epoch 174/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.2443 - accuracy: 0.9209 - val_loss: 0.1777 - val_accuracy: 0.9450\n",
            "Epoch 175/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.2513 - accuracy: 0.9234 - val_loss: 0.1807 - val_accuracy: 0.9450\n",
            "Epoch 176/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2432 - accuracy: 0.9271 - val_loss: 0.1794 - val_accuracy: 0.9400\n",
            "Epoch 177/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2200 - accuracy: 0.9422 - val_loss: 0.1805 - val_accuracy: 0.9400\n",
            "Epoch 178/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.2280 - accuracy: 0.9347 - val_loss: 0.1801 - val_accuracy: 0.9450\n",
            "Epoch 179/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.2286 - accuracy: 0.9246 - val_loss: 0.1778 - val_accuracy: 0.9500\n",
            "Epoch 180/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2031 - accuracy: 0.9435 - val_loss: 0.1765 - val_accuracy: 0.9400\n",
            "Epoch 181/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.2131 - accuracy: 0.9359 - val_loss: 0.1718 - val_accuracy: 0.9450\n",
            "Epoch 182/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.2212 - accuracy: 0.9296 - val_loss: 0.1750 - val_accuracy: 0.9450\n",
            "Epoch 183/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2041 - accuracy: 0.9410 - val_loss: 0.1767 - val_accuracy: 0.9450\n",
            "Epoch 184/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2150 - accuracy: 0.9384 - val_loss: 0.1762 - val_accuracy: 0.9450\n",
            "Epoch 185/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2167 - accuracy: 0.9435 - val_loss: 0.1696 - val_accuracy: 0.9450\n",
            "Epoch 186/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2272 - accuracy: 0.9347 - val_loss: 0.1671 - val_accuracy: 0.9450\n",
            "Epoch 187/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.2161 - accuracy: 0.9246 - val_loss: 0.1697 - val_accuracy: 0.9450\n",
            "Epoch 188/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2198 - accuracy: 0.9334 - val_loss: 0.1704 - val_accuracy: 0.9450\n",
            "Epoch 189/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.2400 - accuracy: 0.9246 - val_loss: 0.1687 - val_accuracy: 0.9400\n",
            "Epoch 190/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2238 - accuracy: 0.9234 - val_loss: 0.1718 - val_accuracy: 0.9450\n",
            "Epoch 191/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2215 - accuracy: 0.9347 - val_loss: 0.1713 - val_accuracy: 0.9450\n",
            "Epoch 192/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.2225 - accuracy: 0.9384 - val_loss: 0.1696 - val_accuracy: 0.9450\n",
            "Epoch 193/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.2274 - accuracy: 0.9284 - val_loss: 0.1686 - val_accuracy: 0.9500\n",
            "Epoch 194/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1969 - accuracy: 0.9397 - val_loss: 0.1691 - val_accuracy: 0.9500\n",
            "Epoch 195/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2173 - accuracy: 0.9322 - val_loss: 0.1672 - val_accuracy: 0.9500\n",
            "Epoch 196/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.2094 - accuracy: 0.9372 - val_loss: 0.1649 - val_accuracy: 0.9450\n",
            "Epoch 197/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1838 - accuracy: 0.9472 - val_loss: 0.1620 - val_accuracy: 0.9500\n",
            "Epoch 198/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2060 - accuracy: 0.9410 - val_loss: 0.1630 - val_accuracy: 0.9450\n",
            "Epoch 199/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.2127 - accuracy: 0.9397 - val_loss: 0.1659 - val_accuracy: 0.9500\n",
            "Epoch 200/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1910 - accuracy: 0.9410 - val_loss: 0.1635 - val_accuracy: 0.9500\n",
            "Epoch 201/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.1962 - accuracy: 0.9422 - val_loss: 0.1623 - val_accuracy: 0.9500\n",
            "Epoch 202/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.1959 - accuracy: 0.9472 - val_loss: 0.1629 - val_accuracy: 0.9500\n",
            "Epoch 203/500\n",
            "796/796 [==============================] - 0s 63us/step - loss: 0.1846 - accuracy: 0.9573 - val_loss: 0.1641 - val_accuracy: 0.9450\n",
            "Epoch 204/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1968 - accuracy: 0.9435 - val_loss: 0.1604 - val_accuracy: 0.9500\n",
            "Epoch 205/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1932 - accuracy: 0.9510 - val_loss: 0.1581 - val_accuracy: 0.9450\n",
            "Epoch 206/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1691 - accuracy: 0.9523 - val_loss: 0.1591 - val_accuracy: 0.9500\n",
            "Epoch 207/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1893 - accuracy: 0.9435 - val_loss: 0.1612 - val_accuracy: 0.9500\n",
            "Epoch 208/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1939 - accuracy: 0.9372 - val_loss: 0.1655 - val_accuracy: 0.9500\n",
            "Epoch 209/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1880 - accuracy: 0.9535 - val_loss: 0.1647 - val_accuracy: 0.9500\n",
            "Epoch 210/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1954 - accuracy: 0.9460 - val_loss: 0.1622 - val_accuracy: 0.9500\n",
            "Epoch 211/500\n",
            "796/796 [==============================] - 0s 64us/step - loss: 0.1885 - accuracy: 0.9435 - val_loss: 0.1585 - val_accuracy: 0.9450\n",
            "Epoch 212/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.1918 - accuracy: 0.9435 - val_loss: 0.1622 - val_accuracy: 0.9500\n",
            "Epoch 213/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1679 - accuracy: 0.9447 - val_loss: 0.1633 - val_accuracy: 0.9450\n",
            "Epoch 214/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1902 - accuracy: 0.9485 - val_loss: 0.1651 - val_accuracy: 0.9450\n",
            "Epoch 215/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.1669 - accuracy: 0.9535 - val_loss: 0.1635 - val_accuracy: 0.9500\n",
            "Epoch 216/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1613 - accuracy: 0.9523 - val_loss: 0.1613 - val_accuracy: 0.9500\n",
            "Epoch 217/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1764 - accuracy: 0.9447 - val_loss: 0.1625 - val_accuracy: 0.9500\n",
            "Epoch 218/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1760 - accuracy: 0.9447 - val_loss: 0.1605 - val_accuracy: 0.9500\n",
            "Epoch 219/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1930 - accuracy: 0.9523 - val_loss: 0.1555 - val_accuracy: 0.9500\n",
            "Epoch 220/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.1741 - accuracy: 0.9598 - val_loss: 0.1615 - val_accuracy: 0.9500\n",
            "Epoch 221/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.1753 - accuracy: 0.9460 - val_loss: 0.1602 - val_accuracy: 0.9500\n",
            "Epoch 222/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.2006 - accuracy: 0.9447 - val_loss: 0.1570 - val_accuracy: 0.9500\n",
            "Epoch 223/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1870 - accuracy: 0.9447 - val_loss: 0.1558 - val_accuracy: 0.9500\n",
            "Epoch 224/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1761 - accuracy: 0.9472 - val_loss: 0.1569 - val_accuracy: 0.9500\n",
            "Epoch 225/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1840 - accuracy: 0.9447 - val_loss: 0.1541 - val_accuracy: 0.9500\n",
            "Epoch 226/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1726 - accuracy: 0.9485 - val_loss: 0.1549 - val_accuracy: 0.9500\n",
            "Epoch 227/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1691 - accuracy: 0.9535 - val_loss: 0.1543 - val_accuracy: 0.9500\n",
            "Epoch 228/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1727 - accuracy: 0.9497 - val_loss: 0.1539 - val_accuracy: 0.9550\n",
            "Epoch 229/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1632 - accuracy: 0.9510 - val_loss: 0.1555 - val_accuracy: 0.9550\n",
            "Epoch 230/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.1669 - accuracy: 0.9472 - val_loss: 0.1527 - val_accuracy: 0.9550\n",
            "Epoch 231/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1795 - accuracy: 0.9447 - val_loss: 0.1518 - val_accuracy: 0.9500\n",
            "Epoch 232/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1635 - accuracy: 0.9472 - val_loss: 0.1541 - val_accuracy: 0.9500\n",
            "Epoch 233/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1670 - accuracy: 0.9510 - val_loss: 0.1493 - val_accuracy: 0.9500\n",
            "Epoch 234/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1722 - accuracy: 0.9510 - val_loss: 0.1458 - val_accuracy: 0.9550\n",
            "Epoch 235/500\n",
            "796/796 [==============================] - 0s 64us/step - loss: 0.1622 - accuracy: 0.9510 - val_loss: 0.1509 - val_accuracy: 0.9500\n",
            "Epoch 236/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1714 - accuracy: 0.9523 - val_loss: 0.1491 - val_accuracy: 0.9500\n",
            "Epoch 237/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1567 - accuracy: 0.9523 - val_loss: 0.1489 - val_accuracy: 0.9500\n",
            "Epoch 238/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.1674 - accuracy: 0.9447 - val_loss: 0.1508 - val_accuracy: 0.9500\n",
            "Epoch 239/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.1702 - accuracy: 0.9422 - val_loss: 0.1502 - val_accuracy: 0.9500\n",
            "Epoch 240/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1818 - accuracy: 0.9359 - val_loss: 0.1513 - val_accuracy: 0.9550\n",
            "Epoch 241/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.1598 - accuracy: 0.9648 - val_loss: 0.1543 - val_accuracy: 0.9500\n",
            "Epoch 242/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.1669 - accuracy: 0.9472 - val_loss: 0.1489 - val_accuracy: 0.9600\n",
            "Epoch 243/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.1428 - accuracy: 0.9548 - val_loss: 0.1433 - val_accuracy: 0.9600\n",
            "Epoch 244/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1534 - accuracy: 0.9560 - val_loss: 0.1462 - val_accuracy: 0.9600\n",
            "Epoch 245/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.1469 - accuracy: 0.9611 - val_loss: 0.1454 - val_accuracy: 0.9550\n",
            "Epoch 246/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1570 - accuracy: 0.9523 - val_loss: 0.1464 - val_accuracy: 0.9550\n",
            "Epoch 247/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1440 - accuracy: 0.9648 - val_loss: 0.1482 - val_accuracy: 0.9550\n",
            "Epoch 248/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1874 - accuracy: 0.9435 - val_loss: 0.1511 - val_accuracy: 0.9550\n",
            "Epoch 249/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1639 - accuracy: 0.9560 - val_loss: 0.1510 - val_accuracy: 0.9450\n",
            "Epoch 250/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1521 - accuracy: 0.9523 - val_loss: 0.1494 - val_accuracy: 0.9500\n",
            "Epoch 251/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1622 - accuracy: 0.9422 - val_loss: 0.1528 - val_accuracy: 0.9550\n",
            "Epoch 252/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1361 - accuracy: 0.9673 - val_loss: 0.1499 - val_accuracy: 0.9550\n",
            "Epoch 253/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1449 - accuracy: 0.9598 - val_loss: 0.1476 - val_accuracy: 0.9550\n",
            "Epoch 254/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.1454 - accuracy: 0.9573 - val_loss: 0.1510 - val_accuracy: 0.9550\n",
            "Epoch 255/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1407 - accuracy: 0.9598 - val_loss: 0.1497 - val_accuracy: 0.9550\n",
            "Epoch 256/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1629 - accuracy: 0.9510 - val_loss: 0.1450 - val_accuracy: 0.9650\n",
            "Epoch 257/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1476 - accuracy: 0.9548 - val_loss: 0.1518 - val_accuracy: 0.9550\n",
            "Epoch 258/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1408 - accuracy: 0.9598 - val_loss: 0.1511 - val_accuracy: 0.9550\n",
            "Epoch 259/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.1314 - accuracy: 0.9673 - val_loss: 0.1452 - val_accuracy: 0.9500\n",
            "Epoch 260/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1376 - accuracy: 0.9585 - val_loss: 0.1431 - val_accuracy: 0.9500\n",
            "Epoch 261/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1471 - accuracy: 0.9585 - val_loss: 0.1455 - val_accuracy: 0.9500\n",
            "Epoch 262/500\n",
            "796/796 [==============================] - 0s 64us/step - loss: 0.1377 - accuracy: 0.9548 - val_loss: 0.1498 - val_accuracy: 0.9550\n",
            "Epoch 263/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1430 - accuracy: 0.9523 - val_loss: 0.1488 - val_accuracy: 0.9500\n",
            "Epoch 264/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.1402 - accuracy: 0.9523 - val_loss: 0.1448 - val_accuracy: 0.9500\n",
            "Epoch 265/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1413 - accuracy: 0.9585 - val_loss: 0.1457 - val_accuracy: 0.9550\n",
            "Epoch 266/500\n",
            "796/796 [==============================] - 0s 66us/step - loss: 0.1552 - accuracy: 0.9535 - val_loss: 0.1459 - val_accuracy: 0.9550\n",
            "Epoch 267/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1530 - accuracy: 0.9497 - val_loss: 0.1475 - val_accuracy: 0.9450\n",
            "Epoch 268/500\n",
            "796/796 [==============================] - 0s 64us/step - loss: 0.1242 - accuracy: 0.9661 - val_loss: 0.1473 - val_accuracy: 0.9450\n",
            "Epoch 269/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.1278 - accuracy: 0.9686 - val_loss: 0.1485 - val_accuracy: 0.9500\n",
            "Epoch 270/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1233 - accuracy: 0.9686 - val_loss: 0.1470 - val_accuracy: 0.9500\n",
            "Epoch 271/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1305 - accuracy: 0.9673 - val_loss: 0.1456 - val_accuracy: 0.9550\n",
            "Epoch 272/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.1460 - accuracy: 0.9535 - val_loss: 0.1478 - val_accuracy: 0.9500\n",
            "Epoch 273/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.1435 - accuracy: 0.9585 - val_loss: 0.1432 - val_accuracy: 0.9500\n",
            "Epoch 274/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.1224 - accuracy: 0.9673 - val_loss: 0.1437 - val_accuracy: 0.9500\n",
            "Epoch 275/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1300 - accuracy: 0.9623 - val_loss: 0.1430 - val_accuracy: 0.9550\n",
            "Epoch 276/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.1383 - accuracy: 0.9548 - val_loss: 0.1400 - val_accuracy: 0.9650\n",
            "Epoch 277/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1552 - accuracy: 0.9560 - val_loss: 0.1406 - val_accuracy: 0.9600\n",
            "Epoch 278/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1297 - accuracy: 0.9673 - val_loss: 0.1388 - val_accuracy: 0.9600\n",
            "Epoch 279/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1330 - accuracy: 0.9585 - val_loss: 0.1376 - val_accuracy: 0.9550\n",
            "Epoch 280/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1202 - accuracy: 0.9686 - val_loss: 0.1400 - val_accuracy: 0.9550\n",
            "Epoch 281/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1148 - accuracy: 0.9736 - val_loss: 0.1400 - val_accuracy: 0.9500\n",
            "Epoch 282/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.1503 - accuracy: 0.9510 - val_loss: 0.1404 - val_accuracy: 0.9550\n",
            "Epoch 283/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.1636 - accuracy: 0.9510 - val_loss: 0.1399 - val_accuracy: 0.9600\n",
            "Epoch 284/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1393 - accuracy: 0.9585 - val_loss: 0.1428 - val_accuracy: 0.9600\n",
            "Epoch 285/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1483 - accuracy: 0.9535 - val_loss: 0.1425 - val_accuracy: 0.9600\n",
            "Epoch 286/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1447 - accuracy: 0.9585 - val_loss: 0.1415 - val_accuracy: 0.9550\n",
            "Epoch 287/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.1426 - accuracy: 0.9548 - val_loss: 0.1415 - val_accuracy: 0.9600\n",
            "Epoch 288/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.1419 - accuracy: 0.9560 - val_loss: 0.1394 - val_accuracy: 0.9600\n",
            "Epoch 289/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1350 - accuracy: 0.9598 - val_loss: 0.1407 - val_accuracy: 0.9550\n",
            "Epoch 290/500\n",
            "796/796 [==============================] - 0s 74us/step - loss: 0.1264 - accuracy: 0.9724 - val_loss: 0.1406 - val_accuracy: 0.9550\n",
            "Epoch 291/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1330 - accuracy: 0.9585 - val_loss: 0.1395 - val_accuracy: 0.9500\n",
            "Epoch 292/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1336 - accuracy: 0.9523 - val_loss: 0.1414 - val_accuracy: 0.9450\n",
            "Epoch 293/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1354 - accuracy: 0.9585 - val_loss: 0.1378 - val_accuracy: 0.9600\n",
            "Epoch 294/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.1496 - accuracy: 0.9648 - val_loss: 0.1353 - val_accuracy: 0.9650\n",
            "Epoch 295/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1264 - accuracy: 0.9698 - val_loss: 0.1387 - val_accuracy: 0.9500\n",
            "Epoch 296/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.1236 - accuracy: 0.9661 - val_loss: 0.1373 - val_accuracy: 0.9550\n",
            "Epoch 297/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1372 - accuracy: 0.9611 - val_loss: 0.1352 - val_accuracy: 0.9600\n",
            "Epoch 298/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.1473 - accuracy: 0.9548 - val_loss: 0.1369 - val_accuracy: 0.9650\n",
            "Epoch 299/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.1188 - accuracy: 0.9661 - val_loss: 0.1385 - val_accuracy: 0.9650\n",
            "Epoch 300/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1065 - accuracy: 0.9749 - val_loss: 0.1353 - val_accuracy: 0.9550\n",
            "Epoch 301/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1333 - accuracy: 0.9560 - val_loss: 0.1368 - val_accuracy: 0.9550\n",
            "Epoch 302/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1146 - accuracy: 0.9648 - val_loss: 0.1365 - val_accuracy: 0.9650\n",
            "Epoch 303/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1324 - accuracy: 0.9636 - val_loss: 0.1351 - val_accuracy: 0.9550\n",
            "Epoch 304/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1182 - accuracy: 0.9661 - val_loss: 0.1341 - val_accuracy: 0.9650\n",
            "Epoch 305/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1221 - accuracy: 0.9661 - val_loss: 0.1376 - val_accuracy: 0.9600\n",
            "Epoch 306/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1165 - accuracy: 0.9711 - val_loss: 0.1391 - val_accuracy: 0.9600\n",
            "Epoch 307/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1295 - accuracy: 0.9661 - val_loss: 0.1403 - val_accuracy: 0.9650\n",
            "Epoch 308/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1333 - accuracy: 0.9535 - val_loss: 0.1408 - val_accuracy: 0.9600\n",
            "Epoch 309/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1167 - accuracy: 0.9711 - val_loss: 0.1378 - val_accuracy: 0.9650\n",
            "Epoch 310/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1010 - accuracy: 0.9749 - val_loss: 0.1391 - val_accuracy: 0.9650\n",
            "Epoch 311/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1088 - accuracy: 0.9698 - val_loss: 0.1338 - val_accuracy: 0.9650\n",
            "Epoch 312/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1285 - accuracy: 0.9724 - val_loss: 0.1386 - val_accuracy: 0.9650\n",
            "Epoch 313/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1078 - accuracy: 0.9724 - val_loss: 0.1387 - val_accuracy: 0.9550\n",
            "Epoch 314/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1279 - accuracy: 0.9636 - val_loss: 0.1338 - val_accuracy: 0.9650\n",
            "Epoch 315/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1137 - accuracy: 0.9698 - val_loss: 0.1359 - val_accuracy: 0.9600\n",
            "Epoch 316/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1167 - accuracy: 0.9673 - val_loss: 0.1330 - val_accuracy: 0.9650\n",
            "Epoch 317/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1146 - accuracy: 0.9711 - val_loss: 0.1349 - val_accuracy: 0.9600\n",
            "Epoch 318/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.1329 - accuracy: 0.9673 - val_loss: 0.1323 - val_accuracy: 0.9600\n",
            "Epoch 319/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1218 - accuracy: 0.9623 - val_loss: 0.1315 - val_accuracy: 0.9650\n",
            "Epoch 320/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.1060 - accuracy: 0.9724 - val_loss: 0.1329 - val_accuracy: 0.9650\n",
            "Epoch 321/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1034 - accuracy: 0.9711 - val_loss: 0.1311 - val_accuracy: 0.9650\n",
            "Epoch 322/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0927 - accuracy: 0.9812 - val_loss: 0.1344 - val_accuracy: 0.9650\n",
            "Epoch 323/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.1019 - accuracy: 0.9698 - val_loss: 0.1339 - val_accuracy: 0.9650\n",
            "Epoch 324/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1137 - accuracy: 0.9661 - val_loss: 0.1328 - val_accuracy: 0.9650\n",
            "Epoch 325/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1099 - accuracy: 0.9724 - val_loss: 0.1296 - val_accuracy: 0.9650\n",
            "Epoch 326/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1063 - accuracy: 0.9711 - val_loss: 0.1297 - val_accuracy: 0.9650\n",
            "Epoch 327/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0935 - accuracy: 0.9799 - val_loss: 0.1297 - val_accuracy: 0.9600\n",
            "Epoch 328/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1044 - accuracy: 0.9724 - val_loss: 0.1352 - val_accuracy: 0.9600\n",
            "Epoch 329/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1312 - accuracy: 0.9636 - val_loss: 0.1343 - val_accuracy: 0.9650\n",
            "Epoch 330/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1003 - accuracy: 0.9724 - val_loss: 0.1311 - val_accuracy: 0.9600\n",
            "Epoch 331/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0859 - accuracy: 0.9799 - val_loss: 0.1312 - val_accuracy: 0.9550\n",
            "Epoch 332/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1010 - accuracy: 0.9799 - val_loss: 0.1309 - val_accuracy: 0.9600\n",
            "Epoch 333/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1117 - accuracy: 0.9711 - val_loss: 0.1295 - val_accuracy: 0.9700\n",
            "Epoch 334/500\n",
            "796/796 [==============================] - 0s 63us/step - loss: 0.1027 - accuracy: 0.9749 - val_loss: 0.1286 - val_accuracy: 0.9650\n",
            "Epoch 335/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.1140 - accuracy: 0.9661 - val_loss: 0.1338 - val_accuracy: 0.9550\n",
            "Epoch 336/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1307 - accuracy: 0.9636 - val_loss: 0.1322 - val_accuracy: 0.9550\n",
            "Epoch 337/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.1016 - accuracy: 0.9761 - val_loss: 0.1335 - val_accuracy: 0.9600\n",
            "Epoch 338/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1045 - accuracy: 0.9736 - val_loss: 0.1363 - val_accuracy: 0.9600\n",
            "Epoch 339/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1052 - accuracy: 0.9774 - val_loss: 0.1287 - val_accuracy: 0.9600\n",
            "Epoch 340/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1035 - accuracy: 0.9736 - val_loss: 0.1257 - val_accuracy: 0.9600\n",
            "Epoch 341/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.1122 - accuracy: 0.9724 - val_loss: 0.1280 - val_accuracy: 0.9600\n",
            "Epoch 342/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0870 - accuracy: 0.9799 - val_loss: 0.1272 - val_accuracy: 0.9600\n",
            "Epoch 343/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1063 - accuracy: 0.9724 - val_loss: 0.1230 - val_accuracy: 0.9600\n",
            "Epoch 344/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1142 - accuracy: 0.9711 - val_loss: 0.1270 - val_accuracy: 0.9650\n",
            "Epoch 345/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.1062 - accuracy: 0.9736 - val_loss: 0.1283 - val_accuracy: 0.9650\n",
            "Epoch 346/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.1285 - accuracy: 0.9573 - val_loss: 0.1288 - val_accuracy: 0.9650\n",
            "Epoch 347/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0917 - accuracy: 0.9824 - val_loss: 0.1280 - val_accuracy: 0.9650\n",
            "Epoch 348/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1078 - accuracy: 0.9623 - val_loss: 0.1307 - val_accuracy: 0.9550\n",
            "Epoch 349/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0920 - accuracy: 0.9749 - val_loss: 0.1298 - val_accuracy: 0.9650\n",
            "Epoch 350/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0990 - accuracy: 0.9749 - val_loss: 0.1314 - val_accuracy: 0.9650\n",
            "Epoch 351/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0931 - accuracy: 0.9711 - val_loss: 0.1348 - val_accuracy: 0.9600\n",
            "Epoch 352/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0895 - accuracy: 0.9774 - val_loss: 0.1349 - val_accuracy: 0.9600\n",
            "Epoch 353/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1140 - accuracy: 0.9661 - val_loss: 0.1315 - val_accuracy: 0.9650\n",
            "Epoch 354/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0961 - accuracy: 0.9686 - val_loss: 0.1312 - val_accuracy: 0.9650\n",
            "Epoch 355/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.1039 - accuracy: 0.9724 - val_loss: 0.1311 - val_accuracy: 0.9600\n",
            "Epoch 356/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0825 - accuracy: 0.9786 - val_loss: 0.1324 - val_accuracy: 0.9600\n",
            "Epoch 357/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1035 - accuracy: 0.9749 - val_loss: 0.1292 - val_accuracy: 0.9600\n",
            "Epoch 358/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0926 - accuracy: 0.9749 - val_loss: 0.1301 - val_accuracy: 0.9650\n",
            "Epoch 359/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0804 - accuracy: 0.9812 - val_loss: 0.1310 - val_accuracy: 0.9650\n",
            "Epoch 360/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0879 - accuracy: 0.9749 - val_loss: 0.1369 - val_accuracy: 0.9600\n",
            "Epoch 361/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0971 - accuracy: 0.9711 - val_loss: 0.1353 - val_accuracy: 0.9600\n",
            "Epoch 362/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0830 - accuracy: 0.9736 - val_loss: 0.1347 - val_accuracy: 0.9500\n",
            "Epoch 363/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1019 - accuracy: 0.9724 - val_loss: 0.1327 - val_accuracy: 0.9600\n",
            "Epoch 364/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0873 - accuracy: 0.9774 - val_loss: 0.1324 - val_accuracy: 0.9550\n",
            "Epoch 365/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.0832 - accuracy: 0.9824 - val_loss: 0.1347 - val_accuracy: 0.9550\n",
            "Epoch 366/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0950 - accuracy: 0.9749 - val_loss: 0.1365 - val_accuracy: 0.9600\n",
            "Epoch 367/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.1248 - accuracy: 0.9598 - val_loss: 0.1356 - val_accuracy: 0.9650\n",
            "Epoch 368/500\n",
            "796/796 [==============================] - 0s 56us/step - loss: 0.0904 - accuracy: 0.9749 - val_loss: 0.1338 - val_accuracy: 0.9650\n",
            "Epoch 369/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0989 - accuracy: 0.9648 - val_loss: 0.1327 - val_accuracy: 0.9650\n",
            "Epoch 370/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0891 - accuracy: 0.9749 - val_loss: 0.1304 - val_accuracy: 0.9600\n",
            "Epoch 371/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1063 - accuracy: 0.9661 - val_loss: 0.1316 - val_accuracy: 0.9650\n",
            "Epoch 372/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0951 - accuracy: 0.9749 - val_loss: 0.1340 - val_accuracy: 0.9650\n",
            "Epoch 373/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0881 - accuracy: 0.9749 - val_loss: 0.1365 - val_accuracy: 0.9650\n",
            "Epoch 374/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0856 - accuracy: 0.9786 - val_loss: 0.1380 - val_accuracy: 0.9650\n",
            "Epoch 375/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.0841 - accuracy: 0.9812 - val_loss: 0.1365 - val_accuracy: 0.9600\n",
            "Epoch 376/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0964 - accuracy: 0.9761 - val_loss: 0.1372 - val_accuracy: 0.9600\n",
            "Epoch 377/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0954 - accuracy: 0.9724 - val_loss: 0.1374 - val_accuracy: 0.9600\n",
            "Epoch 378/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0816 - accuracy: 0.9774 - val_loss: 0.1402 - val_accuracy: 0.9600\n",
            "Epoch 379/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0822 - accuracy: 0.9774 - val_loss: 0.1389 - val_accuracy: 0.9650\n",
            "Epoch 380/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.1038 - accuracy: 0.9736 - val_loss: 0.1370 - val_accuracy: 0.9650\n",
            "Epoch 381/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0941 - accuracy: 0.9736 - val_loss: 0.1363 - val_accuracy: 0.9650\n",
            "Epoch 382/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0961 - accuracy: 0.9724 - val_loss: 0.1376 - val_accuracy: 0.9650\n",
            "Epoch 383/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0689 - accuracy: 0.9837 - val_loss: 0.1361 - val_accuracy: 0.9650\n",
            "Epoch 384/500\n",
            "796/796 [==============================] - 0s 56us/step - loss: 0.0793 - accuracy: 0.9749 - val_loss: 0.1363 - val_accuracy: 0.9650\n",
            "Epoch 385/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0987 - accuracy: 0.9648 - val_loss: 0.1312 - val_accuracy: 0.9650\n",
            "Epoch 386/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0846 - accuracy: 0.9774 - val_loss: 0.1255 - val_accuracy: 0.9650\n",
            "Epoch 387/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1098 - accuracy: 0.9598 - val_loss: 0.1278 - val_accuracy: 0.9650\n",
            "Epoch 388/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0939 - accuracy: 0.9711 - val_loss: 0.1352 - val_accuracy: 0.9650\n",
            "Epoch 389/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.0918 - accuracy: 0.9686 - val_loss: 0.1399 - val_accuracy: 0.9600\n",
            "Epoch 390/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0821 - accuracy: 0.9849 - val_loss: 0.1401 - val_accuracy: 0.9600\n",
            "Epoch 391/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0770 - accuracy: 0.9837 - val_loss: 0.1415 - val_accuracy: 0.9600\n",
            "Epoch 392/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0721 - accuracy: 0.9849 - val_loss: 0.1346 - val_accuracy: 0.9650\n",
            "Epoch 393/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0821 - accuracy: 0.9749 - val_loss: 0.1326 - val_accuracy: 0.9650\n",
            "Epoch 394/500\n",
            "796/796 [==============================] - 0s 64us/step - loss: 0.0890 - accuracy: 0.9761 - val_loss: 0.1361 - val_accuracy: 0.9650\n",
            "Epoch 395/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.0935 - accuracy: 0.9686 - val_loss: 0.1360 - val_accuracy: 0.9600\n",
            "Epoch 396/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0765 - accuracy: 0.9837 - val_loss: 0.1344 - val_accuracy: 0.9600\n",
            "Epoch 397/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0844 - accuracy: 0.9837 - val_loss: 0.1340 - val_accuracy: 0.9600\n",
            "Epoch 398/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0689 - accuracy: 0.9874 - val_loss: 0.1341 - val_accuracy: 0.9600\n",
            "Epoch 399/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0898 - accuracy: 0.9749 - val_loss: 0.1330 - val_accuracy: 0.9650\n",
            "Epoch 400/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0930 - accuracy: 0.9724 - val_loss: 0.1339 - val_accuracy: 0.9600\n",
            "Epoch 401/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0802 - accuracy: 0.9761 - val_loss: 0.1354 - val_accuracy: 0.9650\n",
            "Epoch 402/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0740 - accuracy: 0.9824 - val_loss: 0.1364 - val_accuracy: 0.9600\n",
            "Epoch 403/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.1041 - accuracy: 0.9749 - val_loss: 0.1356 - val_accuracy: 0.9650\n",
            "Epoch 404/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0706 - accuracy: 0.9812 - val_loss: 0.1312 - val_accuracy: 0.9700\n",
            "Epoch 405/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0852 - accuracy: 0.9724 - val_loss: 0.1308 - val_accuracy: 0.9650\n",
            "Epoch 406/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0791 - accuracy: 0.9749 - val_loss: 0.1319 - val_accuracy: 0.9600\n",
            "Epoch 407/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0811 - accuracy: 0.9724 - val_loss: 0.1323 - val_accuracy: 0.9650\n",
            "Epoch 408/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0614 - accuracy: 0.9887 - val_loss: 0.1311 - val_accuracy: 0.9650\n",
            "Epoch 409/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0673 - accuracy: 0.9774 - val_loss: 0.1310 - val_accuracy: 0.9600\n",
            "Epoch 410/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0731 - accuracy: 0.9824 - val_loss: 0.1304 - val_accuracy: 0.9550\n",
            "Epoch 411/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0821 - accuracy: 0.9749 - val_loss: 0.1308 - val_accuracy: 0.9550\n",
            "Epoch 412/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0834 - accuracy: 0.9799 - val_loss: 0.1343 - val_accuracy: 0.9600\n",
            "Epoch 413/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0748 - accuracy: 0.9812 - val_loss: 0.1317 - val_accuracy: 0.9600\n",
            "Epoch 414/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0864 - accuracy: 0.9711 - val_loss: 0.1315 - val_accuracy: 0.9650\n",
            "Epoch 415/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0864 - accuracy: 0.9711 - val_loss: 0.1340 - val_accuracy: 0.9650\n",
            "Epoch 416/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0683 - accuracy: 0.9849 - val_loss: 0.1357 - val_accuracy: 0.9600\n",
            "Epoch 417/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0728 - accuracy: 0.9799 - val_loss: 0.1378 - val_accuracy: 0.9600\n",
            "Epoch 418/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0656 - accuracy: 0.9887 - val_loss: 0.1351 - val_accuracy: 0.9600\n",
            "Epoch 419/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0722 - accuracy: 0.9799 - val_loss: 0.1420 - val_accuracy: 0.9550\n",
            "Epoch 420/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0707 - accuracy: 0.9786 - val_loss: 0.1426 - val_accuracy: 0.9550\n",
            "Epoch 421/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.1001 - accuracy: 0.9648 - val_loss: 0.1404 - val_accuracy: 0.9550\n",
            "Epoch 422/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0710 - accuracy: 0.9837 - val_loss: 0.1369 - val_accuracy: 0.9600\n",
            "Epoch 423/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0785 - accuracy: 0.9749 - val_loss: 0.1371 - val_accuracy: 0.9600\n",
            "Epoch 424/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0726 - accuracy: 0.9786 - val_loss: 0.1397 - val_accuracy: 0.9600\n",
            "Epoch 425/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0653 - accuracy: 0.9849 - val_loss: 0.1373 - val_accuracy: 0.9600\n",
            "Epoch 426/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0749 - accuracy: 0.9749 - val_loss: 0.1385 - val_accuracy: 0.9550\n",
            "Epoch 427/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0781 - accuracy: 0.9774 - val_loss: 0.1411 - val_accuracy: 0.9550\n",
            "Epoch 428/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0686 - accuracy: 0.9849 - val_loss: 0.1421 - val_accuracy: 0.9550\n",
            "Epoch 429/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0882 - accuracy: 0.9698 - val_loss: 0.1395 - val_accuracy: 0.9600\n",
            "Epoch 430/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0821 - accuracy: 0.9724 - val_loss: 0.1366 - val_accuracy: 0.9600\n",
            "Epoch 431/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0792 - accuracy: 0.9786 - val_loss: 0.1407 - val_accuracy: 0.9600\n",
            "Epoch 432/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0891 - accuracy: 0.9761 - val_loss: 0.1415 - val_accuracy: 0.9600\n",
            "Epoch 433/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0763 - accuracy: 0.9774 - val_loss: 0.1409 - val_accuracy: 0.9600\n",
            "Epoch 434/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0803 - accuracy: 0.9749 - val_loss: 0.1373 - val_accuracy: 0.9600\n",
            "Epoch 435/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0858 - accuracy: 0.9736 - val_loss: 0.1376 - val_accuracy: 0.9600\n",
            "Epoch 436/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0823 - accuracy: 0.9749 - val_loss: 0.1366 - val_accuracy: 0.9650\n",
            "Epoch 437/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0696 - accuracy: 0.9862 - val_loss: 0.1383 - val_accuracy: 0.9650\n",
            "Epoch 438/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0811 - accuracy: 0.9749 - val_loss: 0.1385 - val_accuracy: 0.9650\n",
            "Epoch 439/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0766 - accuracy: 0.9761 - val_loss: 0.1371 - val_accuracy: 0.9600\n",
            "Epoch 440/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0824 - accuracy: 0.9786 - val_loss: 0.1370 - val_accuracy: 0.9650\n",
            "Epoch 441/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0712 - accuracy: 0.9849 - val_loss: 0.1388 - val_accuracy: 0.9650\n",
            "Epoch 442/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0821 - accuracy: 0.9774 - val_loss: 0.1436 - val_accuracy: 0.9600\n",
            "Epoch 443/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0778 - accuracy: 0.9774 - val_loss: 0.1441 - val_accuracy: 0.9600\n",
            "Epoch 444/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.0721 - accuracy: 0.9786 - val_loss: 0.1387 - val_accuracy: 0.9600\n",
            "Epoch 445/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0705 - accuracy: 0.9824 - val_loss: 0.1355 - val_accuracy: 0.9650\n",
            "Epoch 446/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0661 - accuracy: 0.9837 - val_loss: 0.1314 - val_accuracy: 0.9650\n",
            "Epoch 447/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0609 - accuracy: 0.9837 - val_loss: 0.1309 - val_accuracy: 0.9650\n",
            "Epoch 448/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0842 - accuracy: 0.9786 - val_loss: 0.1350 - val_accuracy: 0.9600\n",
            "Epoch 449/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0448 - accuracy: 0.9962 - val_loss: 0.1355 - val_accuracy: 0.9650\n",
            "Epoch 450/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0836 - accuracy: 0.9686 - val_loss: 0.1391 - val_accuracy: 0.9650\n",
            "Epoch 451/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0606 - accuracy: 0.9812 - val_loss: 0.1430 - val_accuracy: 0.9650\n",
            "Epoch 452/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0826 - accuracy: 0.9824 - val_loss: 0.1387 - val_accuracy: 0.9650\n",
            "Epoch 453/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0724 - accuracy: 0.9786 - val_loss: 0.1338 - val_accuracy: 0.9650\n",
            "Epoch 454/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0690 - accuracy: 0.9799 - val_loss: 0.1381 - val_accuracy: 0.9600\n",
            "Epoch 455/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0719 - accuracy: 0.9736 - val_loss: 0.1375 - val_accuracy: 0.9650\n",
            "Epoch 456/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0615 - accuracy: 0.9849 - val_loss: 0.1368 - val_accuracy: 0.9600\n",
            "Epoch 457/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0696 - accuracy: 0.9761 - val_loss: 0.1387 - val_accuracy: 0.9550\n",
            "Epoch 458/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0622 - accuracy: 0.9874 - val_loss: 0.1359 - val_accuracy: 0.9500\n",
            "Epoch 459/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0694 - accuracy: 0.9862 - val_loss: 0.1350 - val_accuracy: 0.9600\n",
            "Epoch 460/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0652 - accuracy: 0.9849 - val_loss: 0.1375 - val_accuracy: 0.9650\n",
            "Epoch 461/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0642 - accuracy: 0.9887 - val_loss: 0.1412 - val_accuracy: 0.9600\n",
            "Epoch 462/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0935 - accuracy: 0.9698 - val_loss: 0.1391 - val_accuracy: 0.9600\n",
            "Epoch 463/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0695 - accuracy: 0.9824 - val_loss: 0.1414 - val_accuracy: 0.9600\n",
            "Epoch 464/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.0698 - accuracy: 0.9774 - val_loss: 0.1445 - val_accuracy: 0.9600\n",
            "Epoch 465/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0597 - accuracy: 0.9874 - val_loss: 0.1423 - val_accuracy: 0.9650\n",
            "Epoch 466/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0571 - accuracy: 0.9874 - val_loss: 0.1373 - val_accuracy: 0.9650\n",
            "Epoch 467/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0794 - accuracy: 0.9786 - val_loss: 0.1355 - val_accuracy: 0.9650\n",
            "Epoch 468/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0693 - accuracy: 0.9862 - val_loss: 0.1395 - val_accuracy: 0.9650\n",
            "Epoch 469/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0762 - accuracy: 0.9749 - val_loss: 0.1411 - val_accuracy: 0.9650\n",
            "Epoch 470/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0679 - accuracy: 0.9824 - val_loss: 0.1400 - val_accuracy: 0.9650\n",
            "Epoch 471/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0679 - accuracy: 0.9824 - val_loss: 0.1350 - val_accuracy: 0.9600\n",
            "Epoch 472/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0576 - accuracy: 0.9874 - val_loss: 0.1327 - val_accuracy: 0.9600\n",
            "Epoch 473/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0703 - accuracy: 0.9837 - val_loss: 0.1339 - val_accuracy: 0.9600\n",
            "Epoch 474/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0569 - accuracy: 0.9837 - val_loss: 0.1350 - val_accuracy: 0.9600\n",
            "Epoch 475/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0666 - accuracy: 0.9812 - val_loss: 0.1382 - val_accuracy: 0.9600\n",
            "Epoch 476/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0724 - accuracy: 0.9786 - val_loss: 0.1374 - val_accuracy: 0.9600\n",
            "Epoch 477/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0682 - accuracy: 0.9812 - val_loss: 0.1344 - val_accuracy: 0.9600\n",
            "Epoch 478/500\n",
            "796/796 [==============================] - 0s 57us/step - loss: 0.0633 - accuracy: 0.9862 - val_loss: 0.1348 - val_accuracy: 0.9600\n",
            "Epoch 479/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0588 - accuracy: 0.9849 - val_loss: 0.1322 - val_accuracy: 0.9600\n",
            "Epoch 480/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0768 - accuracy: 0.9824 - val_loss: 0.1289 - val_accuracy: 0.9650\n",
            "Epoch 481/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0709 - accuracy: 0.9761 - val_loss: 0.1265 - val_accuracy: 0.9600\n",
            "Epoch 482/500\n",
            "796/796 [==============================] - 0s 60us/step - loss: 0.0695 - accuracy: 0.9849 - val_loss: 0.1270 - val_accuracy: 0.9650\n",
            "Epoch 483/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0564 - accuracy: 0.9874 - val_loss: 0.1308 - val_accuracy: 0.9650\n",
            "Epoch 484/500\n",
            "796/796 [==============================] - 0s 64us/step - loss: 0.0639 - accuracy: 0.9786 - val_loss: 0.1345 - val_accuracy: 0.9650\n",
            "Epoch 485/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0604 - accuracy: 0.9824 - val_loss: 0.1366 - val_accuracy: 0.9650\n",
            "Epoch 486/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.0562 - accuracy: 0.9837 - val_loss: 0.1314 - val_accuracy: 0.9600\n",
            "Epoch 487/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.0719 - accuracy: 0.9774 - val_loss: 0.1305 - val_accuracy: 0.9600\n",
            "Epoch 488/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0546 - accuracy: 0.9899 - val_loss: 0.1282 - val_accuracy: 0.9600\n",
            "Epoch 489/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0763 - accuracy: 0.9761 - val_loss: 0.1306 - val_accuracy: 0.9650\n",
            "Epoch 490/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0603 - accuracy: 0.9837 - val_loss: 0.1333 - val_accuracy: 0.9650\n",
            "Epoch 491/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0597 - accuracy: 0.9812 - val_loss: 0.1344 - val_accuracy: 0.9600\n",
            "Epoch 492/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0714 - accuracy: 0.9812 - val_loss: 0.1366 - val_accuracy: 0.9600\n",
            "Epoch 493/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.0704 - accuracy: 0.9774 - val_loss: 0.1365 - val_accuracy: 0.9600\n",
            "Epoch 494/500\n",
            "796/796 [==============================] - 0s 58us/step - loss: 0.0595 - accuracy: 0.9812 - val_loss: 0.1386 - val_accuracy: 0.9600\n",
            "Epoch 495/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.0618 - accuracy: 0.9837 - val_loss: 0.1355 - val_accuracy: 0.9600\n",
            "Epoch 496/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.0649 - accuracy: 0.9724 - val_loss: 0.1344 - val_accuracy: 0.9600\n",
            "Epoch 497/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.0562 - accuracy: 0.9849 - val_loss: 0.1308 - val_accuracy: 0.9600\n",
            "Epoch 498/500\n",
            "796/796 [==============================] - 0s 62us/step - loss: 0.0615 - accuracy: 0.9799 - val_loss: 0.1308 - val_accuracy: 0.9600\n",
            "Epoch 499/500\n",
            "796/796 [==============================] - 0s 59us/step - loss: 0.0748 - accuracy: 0.9824 - val_loss: 0.1326 - val_accuracy: 0.9600\n",
            "Epoch 500/500\n",
            "796/796 [==============================] - 0s 61us/step - loss: 0.0481 - accuracy: 0.9912 - val_loss: 0.1343 - val_accuracy: 0.9600\n",
            "Validation loss: 0.13428444683551788\n",
            "Validation accuracy: 0.9599999785423279\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz7uYsPgcQCw"
      },
      "source": [
        "\n",
        "\n",
        "#### save the model and weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj9mhjwXcPcY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b36af91c-c5a8-4ddf-9eae-8f9712b08183"
      },
      "source": [
        "# The following lines will help you to save the model and the weights. Because remember that every time you train your model the initialization is random then you will have different results.\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"/content/drive/My Drive/MLCV/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights(\"/content/drive/My Drive/MLCV/model_weights.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}